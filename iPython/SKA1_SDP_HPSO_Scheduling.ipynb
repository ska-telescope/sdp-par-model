{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.3.1 running Python 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "\n",
    "from sdp_par_model.scheduler import Definitions as sdefs\n",
    "from sdp_par_model.scheduler import Scheduler\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "import bisect\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a sequence of tasks \n",
    "### (using letters A..G to define scheduling  blocks - see Google Drive or Python code  for Definitions)\n",
    "### This can also be read from e.g. a CSV file if you want flexible schedules to be simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_capacity_low = 13.8  # PetaFlops\n",
    "flops_capacity_mid = 12.1  # PetaFlops\n",
    "\n",
    "cold_buffer_size_low = 30 # PetaBytes\n",
    "hot_buffer_size_low  = 20  # PetaBytes\n",
    "\n",
    "cold_buffer_size_mid = 30 # PetaBytes\n",
    "hot_buffer_size_mid  = 20  # PetaBytes\n",
    "\n",
    "\n",
    "seqL = ['B','A','A',] + ['B',]*32 + ['A',]*2 + ['B',]*73 + ['A',] + ['B',]*43\n",
    "seqM = ['B','G',] + ['B',]*34 + ['G','C','F',] + ['B',]*110 +['F',]*91 + ['G',]*2 + ['E',]*4 + ['D',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Scheduler object, sets it up, execute the scheduler, and plot the results\n",
    "At the moment the Scheduler object contains the functionality of the SDP simulator, the scheduling code itself, as well as the generated schedule objects. In future we may wish to split them into separate classes and objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sequence_to_simulate = seqL\n",
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "tel_str = \"LOW\"\n",
    "\n",
    "# Randomly shuffles the sequence of processing blocks (letters)\n",
    "random.shuffle(sequence_to_simulate)\n",
    "\n",
    "sdp_scheduler = Scheduler()\n",
    "\n",
    "## Read  performace requirement lookup for all HPSOs. \n",
    "### If this lookup table does not exist, we create it, and save it to disk (to save time re-computing)\n",
    "performance_lookup_filename = \"performance_dict.data\"\n",
    "if os.path.isfile(performance_lookup_filename):\n",
    "    performance_dict = None\n",
    "    with open(performance_lookup_filename, \"rb\") as f:\n",
    "        performance_dict = pickle.load(f)\n",
    "    sdp_scheduler.set_performance_dictionary(performance_dict)\n",
    "else:\n",
    "    # Create a performance dictionary and write it to file\n",
    "    performance_dict = sdp_scheduler.compute_performance_dictionary()\n",
    "    with open(performance_lookup_filename, \"wb\") as f:\n",
    "        pickle.dump(performance_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# TODO: ALLOW STREAMING FROM COLD TO HOT BUFFER\n",
    "\n",
    "task_list = sdp_scheduler.task_letters_to_sdp_task_list(sequence_to_simulate)\n",
    "'''To show how the tasks are created, can print the sequence of Task objects.'''\n",
    "\n",
    "#for task in task_list:\n",
    "#    print(task)\n",
    "\n",
    "schedule = sdp_scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                                  assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "last_preservation_timestamp = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "max_t = last_preservation_timestamp\n",
    "print(\"SDP task sequence completes at t = %g hrs\" % (max_t / 3600))\n",
    "\n",
    "# Now we plot the results\n",
    "\n",
    "max_preservation = sorted(schedule.preserve_deltas.values())[-1]\n",
    "last_preservation_timestamp = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "xrange = [0, last_preservation_timestamp * 1.05]\n",
    "preserv_yrange = [0, max(max_preservation * 1.05, 1)]\n",
    "\n",
    "max_t = last_preservation_timestamp\n",
    "\n",
    "iapi.plot_deltas(schedule.flops_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP FLOP/s (capped at %.3g PetaFLOPS)' % (tel_str, flops_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaFLOP/s')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.memory_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Evolution of SDP working memory (RAM)', xlabel='wall clock time (hours)', ylabel='TeraByte')\n",
    "'''                 \n",
    "iapi.plot_deltas(schedule.cold_buffer_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP Cold buffer usage (capped at %.3g PetaByte)' % (tel_str, coldbuf_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.hot_buffer_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP Hot Buffer usage (capped at %.0f PetaByte)' % (tel_str, hotbuf_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.preserve_deltas, xrange=xrange, yrange=preserv_yrange, max_t=max_t, \n",
    "                 title='%s SDP Preservation usage (uncapped)' % tel_str, xlabel='wall clock time (hours)', ylabel='TeraByte')\n",
    "\n",
    "iapi.plot_deltas(schedule.ingest_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Ingest pipeline -> Cold Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')                 \n",
    "'''\n",
    "# Ingest -> Working memory pipeline is identical to Working Memory -> Cold Buffer (streaming)\n",
    "iapi.plot_deltas(schedule.mem_cold_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of Ingest working memory -> Cold Buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.cold_hot_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Cold Buffer -> Hot Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.hot_mem_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from hot buffer to working memory', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "iapi.plot_deltas(schedule.mem_hot_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from working memory to hot buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''                 \n",
    "iapi.plot_deltas(schedule.hot_preserve_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth %s (Hot Buffer -> Preservation)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a number of randomized sequences, looking at spread of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_of_runs = 100\n",
    "\n",
    "sequence_to_simulate = seqL\n",
    "\n",
    "# Caps are all in \"Peta\" units (FLOPS, or Bytes)\n",
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "tel_str = \"LOW\"\n",
    "\n",
    "## Read  performace requirement lookup for all HPSOs. \n",
    "### If this lookup table does not exist, we create it, and save it to disk (to save time re-computing)\n",
    "performance_lookup_filename = \"performance_dict.data\"\n",
    "performance_dict = None\n",
    "if os.path.isfile(performance_lookup_filename):\n",
    "    performance_dict = None\n",
    "    with open(performance_lookup_filename, \"rb\") as f:\n",
    "        performance_dict = pickle.load(f)\n",
    "else:\n",
    "    raise Exception(\"No Performance dictionary file found!\")\n",
    "\n",
    "runtimes = np.zeros(nr_of_runs)\n",
    "for i in range(nr_of_runs):\n",
    "    # Randomly shuffles the sequence of processing blocks (letters)\n",
    "    random.shuffle(sequence_to_simulate)\n",
    "    sdp_scheduler = Scheduler()  # Create a new scheduler, because the Scheduler has a state\n",
    "    sdp_scheduler.set_performance_dictionary(performance_dict)\n",
    "    task_list = sdp_scheduler.task_letters_to_sdp_task_list(sequence_to_simulate)\n",
    "\n",
    "    schedule = sdp_scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                                      assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "    max_t = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "    runtimes[i] = max_t\n",
    "    print(\"Run %d of %d : SDP task seq completed at t = %g hrs\" % (i+1, nr_of_runs, (max_t / 3600)))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(runtimes/3600)\n",
    "plt.title('Distribution of execution times (median = %.1f hours)' % np.median(runtimes/3600), Fontsize=20)\n",
    "plt.xlabel('Hours', Fontsize=16)\n",
    "plt.ylabel('Nr of occurrences', Fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to approximately replicate Rosie's results. Check (rerun) the notebook at that repository revision to regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.0.0 running Python 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model import evaluate as imp\n",
    "from sdp_par_model.config import PipelineConfig\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "import numpy as np\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structures and methods for handling SDP tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs some refactoring methinks; idea would be to specify HPSOs instead of \"letters\". \n",
    "hpso_lookup = {'A' : HPSOs.hpso01, \n",
    "               'B' : HPSOs.hpso04c,  # TODO: This task not properly defined yet\n",
    "               'C' : HPSOs.hpso13, \n",
    "               'D' : HPSOs.hpso14,\n",
    "               'E' : HPSOs.hpso15,\n",
    "               'F' : HPSOs.hpso27,\n",
    "               'G' : HPSOs.hpso37c}\n",
    "\n",
    "# The following results map was copied from examples used by Peter Wortmann. It defines values we wish to calculate.\n",
    "#               Title                      Unit       Default? Sum?             Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Rate',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "\n",
    "class SDPTask:\n",
    "    uid          = None  # Optional: unique ID; can be used for sequencing\n",
    "    description  = None  # Provides a description\n",
    "    t_min_start  = None  # Earliest wall clock time (in seconds) that this task can / may start\n",
    "    prec_task    = None  # Preceding task (uid) that needs to complete before this one can start\n",
    "    t_fixed      = None  # fixed minimum duration (in seconds) of this task (e.g. for an observation)\n",
    "    data_in      = None  # Amount of data (in TB) this task requires *before* starting (usually read from hot buffer)\n",
    "    memsize      = None  # The amount of SDP working memory (in TB) needed to perform this task\n",
    "    flopcount    = None  # Number of operations (in PetaFLOP) required to complete this task\n",
    "    data_out     = None  # Amount of data (in TB) this task stores *after* finishing (usually written to hot buffer)    \n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"SDPTask (type undefined): \"\n",
    "        if self.description is not None:\n",
    "            s = \"SDPTask (%s):\" % self.description\n",
    "        fields = self.__dict__\n",
    "        for k in fields.keys():\n",
    "            key_string = str(k)\n",
    "            value_string = str(fields[k])\n",
    "            if len(value_string) > 40:\n",
    "                value_string = value_string[:40] + \"... (truncated)\"\n",
    "            s += \"\\n %s\\t\\t= %s\" % (key_string, value_string)\n",
    "        return s\n",
    "            \n",
    "def task_letters_to_SDPTask_list(letter_sequence, performance_dict):\n",
    "    \"\"\"\n",
    "    Converts a list of task letters into a list of SDPTask objects\n",
    "    @param letter_sequence : a sequence of HPSOs, defined by Rosie's A..G lettering scheme. TODO: replace by actual HPSO names\n",
    "    @param performance_dict : a dictionary of computational requirements for each HPSO; these need to be computed only once\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    uid =  -1\n",
    "    for task_letter in letter_sequence:\n",
    "        hpso = hpso_lookup[task_letter]\n",
    "        hpso_subtasks = HPSOs.hpso_subtasks[hpso]\n",
    "        nr_subtasks = len(hpso_subtasks)\n",
    "        assert nr_subtasks > 2\n",
    "\n",
    "        if not (hpso_subtasks[0] in HPSOs.ingest_subtasks) and (hpso_subtasks[1] in HPSOs.rcal_subtasks):\n",
    "            # this is assumed true for all HPSOs - hence raising an assertion error if not\n",
    "            raise AssertionError(\"Assumption was wrong - some HPSO apparently doesn't involve Ingest + RCal\")\n",
    "        \n",
    "        # Ingest and Rcal are combined into a a single task object, as they cannot be separated         \n",
    "        t = SDPTask()\n",
    "        uid += 1  # the unique id of the combined Ingest+Rcal task\n",
    "        ingest_uid = uid  # remembered for later; some subtasks may only start once this one is completed\n",
    "        t.uid = uid\n",
    "        t.description = 'Ingest+RCal'\n",
    "        subtask = hpso_subtasks[0]  # ingest\n",
    "        t.t_fixed = performance_dict[hpso]['Tobs']\n",
    "        t.data_in  = 0  # data is acquired in real time; nothing read from buffer\n",
    "        t.memsize = performance_dict[hpso][subtask]['cache']\n",
    "        t.flopcount = t.t_fixed * performance_dict[hpso][subtask]['compRate']\n",
    "        t.data_out  = t.t_fixed * performance_dict[hpso][subtask]['ingestRate']  # All ingested data gets written to buffer?\n",
    "\n",
    "        subtask = hpso_subtasks[1]  # rcal\n",
    "        t.memsize += performance_dict[hpso][subtask]['cache']\n",
    "        t.flopcount += t.t_fixed * performance_dict[hpso][subtask]['compRate']\n",
    "        t.data_out  += 0 # What output does RCal generate? Just set it to zero for now. Are ingestRate and visRate relevant?\n",
    "        ingest_rcal_data = t.data_out  # This is the data generated by this combined task; needed by subsequent tasks\n",
    "\n",
    "        tasks.append(t)\n",
    "\n",
    "        # Now handle the rest of the subtasks\n",
    "        for i in range(2, nr_subtasks):\n",
    "            subtask = hpso_subtasks[i]\n",
    "            uid += 1\n",
    "            t = SDPTask()\n",
    "            t.uid = uid\n",
    "            t.description = str(subtask)\n",
    "            t.prec_task = ingest_uid  # must be after ingest+rcal, otherwise not in specific order rel to other subtasks            \n",
    "            t.data_in  = ingest_rcal_data  # TODO: value? read from buffer at a later time than ingest happened at\n",
    "            t.memsize = performance_dict[hpso][subtask]['cache']\n",
    "            # TODO: is the line below correct? Can probably define directly in terms of amount of data instead of via Tobs\n",
    "            t.flopcount = performance_dict[hpso]['Tobs'] * performance_dict[hpso][subtask]['compRate']\n",
    "            t.data_out  = 0  # TODO: No idea what gets output here. visRate? Temporarily set to zero \n",
    "            \n",
    "            tasks.append(t)\n",
    "    return tasks\n",
    "\n",
    "def add_delta(deltas, t, delta):\n",
    "    \"\"\"\n",
    "    Adds a {timestamp : delta} pair to the supplied \"deltas\" dictionary.\n",
    "    If the supplied timestamp value already exists, delta is added to the existig value\n",
    "    \"\"\"\n",
    "    if t in deltas:\n",
    "        warnings.warn('Timestamp entry already exists in the timeline')  # warning may be omitted\n",
    "        deltas[t] += delta\n",
    "    else:\n",
    "        deltas[t] = delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes  performace requirements for each HPSO using parametric model\n",
    "### We do this once, and store the results in a dictionary for lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance_dict = {}  # A dictionary of dictionaries.  HPSO requirements are computed once and stored as lookups\n",
    "\n",
    "# As a test we loop over all HPSOs we wish to handle, computing results for each\n",
    "for task_letter in sorted(hpso_lookup.keys()):\n",
    "    hpso = hpso_lookup[task_letter]\n",
    "    print('*** Processing task type %s => %s ***\\n' % (task_letter, hpso))\n",
    "    if not hpso in performance_dict:\n",
    "        performance_dict[hpso] = {}\n",
    "        \n",
    "    for subtask in HPSOs.hpso_subtasks[hpso]:\n",
    "        print('subtask -> %s' % subtask)\n",
    "        if not subtask in performance_dict[hpso]:\n",
    "            performance_dict[hpso][subtask] = {}\n",
    "        \n",
    "        cfg = PipelineConfig(hpso=hpso, hpso_subtask=subtask)\n",
    "        (valid, msgs) = cfg.is_valid()\n",
    "        if not valid:\n",
    "            print(\"Invalid configuration!\")\n",
    "            for msg in msgs:\n",
    "                print(msg)\n",
    "            raise AssertionError(\"Invalid config\")\n",
    "        tp = cfg.calc_tel_params()\n",
    "        results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "        \n",
    "        performance_dict[hpso]['Tobs'] = tp.Tobs  # Observation time\n",
    "        performance_dict[hpso][subtask]['ingestRate'] = results[0]\n",
    "        performance_dict[hpso][subtask]['cache'] = results[1]\n",
    "        performance_dict[hpso][subtask]['visRate'] = results[2]\n",
    "        performance_dict[hpso][subtask]['compRate'] = results[3]\n",
    "        \n",
    "        print('Buffer ingest rate\\t= %g TB/s' % results[0])\n",
    "        print('Cache memory\\t= %g TB' % results[1])\n",
    "        print('Visibility IO rate\\t= %g TB/s' % results[2])\n",
    "        print('Compute Rate\\t= %g PetaFLOP/s' % results[3])\n",
    "        print()\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create run a short test sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = ('A','A','B','B','B','A')\n",
    "task_list = task_letters_to_SDPTask_list(test_seq, performance_dict)\n",
    "for task in task_list:\n",
    "    print(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook incomplete beyond this point - don't execute unless you know what you're doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, simulate the execution of this sequence on the SDP (incomplete!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "t[1] = 'a'\n",
    "t[3] = 'b'\n",
    "t.pop(3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "# First, assert that the SDP has enough FLOP/s capacity to handle the real-time tasks. If not, we can't continue.\n",
    "tasks_to_be_scheduled = {}  # TODO: we can replace this dictionary with a set once hash method for SDPTask is defined\n",
    "for task in task_list:\n",
    "    tasks_to_be_scheduled[task.uid] = task\n",
    "    if task.t_fixed is not None:\n",
    "        required_FLOPs = task.flopcount / task.t_fixed\n",
    "        #print(\"Task %d requires a FLOPS rate of %g PetaFLOP/s\" % (task.uid, required_FLOPs))\n",
    "        if (task.flopcount / task.t_fixed) > sdp_FLOPS:\n",
    "            raise AssertionError(\"Task %d (%s) requires %g PetaFLOP/s. SDP capacity of %g PetaFLOP/s is insufficient!\" \n",
    "                                  % (task.uid, task.description, required_FLOPs, sdp_FLOPS) )\n",
    "            \n",
    "# Next, run through task list, determining start and end times for their execution and the effect on the buffer\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary that maps wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "tasks_scheduled = {}  # set of task uids; TODO: replace by set of SDPTask objects when hash and equals methods defined\n",
    "\n",
    "nr_iterations = 0\n",
    "max_nr_iterations = 100  # Emergency termination if the loop fails to schedule all taks\n",
    "# Iteratively run through all tasks, scheduling them where possible. Repeat until all tasks are scheduled.\n",
    "while len(tasks_scheduled) > 0:\n",
    "    nr_iterations += 1\n",
    "    if nr_iterations > max_nr_iterations:\n",
    "        warnings.warn('Maximum number of iterations exceeded; aborting!')\n",
    "        break\n",
    "    \n",
    "    for task in task_list:\n",
    "        if (task.prec_task_uid is not None) and (task.uid not in )\n",
    "    \n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "for task in tasks:\n",
    "    if hasattr(task, )\n",
    "\n",
    "# Run through the list of tasks, determining start and end times for their execution and the effect on the buffer\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary mapping wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "uids_completed = set()\n",
    "\n",
    "for task in tasks:\n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n",
    "\n",
    "buffer_evolution = collections.OrderedDict(sorted(buffer_deltas.items()))\n",
    "time_vals   = np.zeros(2 * len(buffer_evolution))\n",
    "buffer_vals = np.zeros(2 * len(buffer_evolution))\n",
    "\n",
    "i = 0\n",
    "buffer_val = 0\n",
    "time_val   = 0\n",
    "for k, delta in buffer_evolution.items(): \n",
    "    #print('(%.1f,\\t%.2f)' % (k/3600, delta))\n",
    "    time_val = k / 3600  # hours\n",
    "    time_vals[i]     = time_val\n",
    "    buffer_vals[i]   = buffer_val\n",
    "    buffer_val += delta  # Adds the buffer delta to the buffer's stored contents\n",
    "    time_vals[i+1]   = time_val  # we assume no time went by (writing being instantaneous)\n",
    "    buffer_vals[i+1] = buffer_val  # TeraBytes\n",
    "    i += 2\n",
    "\n",
    "plt.plot(time_vals, buffer_vals / 1e3, 'b-')\n",
    "plt.title('Evolution of the SDP Buffer while executing the supplied LOW sequence.\\nObservation time = %.1f hrs.' \n",
    "          ' Total execution time = %.1f hrs; Max buffer = %.1f PB' % (wall_clock / 3600, time_vals[-1],                                                                      np.max(buffer_vals)/1e3))\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('buffer usage (PB)')\n",
    "plt.xlim(0, time_vals[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(range(len(idle_time_durations))), idle_time_durations / 3600, marker='s', color = 'r', linewidth=0)\n",
    "plt.title('Idle time that tasks spend in the LOW buffer.\\nSummed idle time for all tasks = %.1f hrs.' % \n",
    "          (np.sum(idle_time_durations) / 3600))\n",
    "plt.xlabel('Task''s number in sequence')\n",
    "plt.ylabel('Time (hours)')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to approximately replicate Rosie's results. Check (rerun) the notebook at that repository revision to regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours -- TODO check whether correct\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of \"Low\" and \"Mid\" sequences from Rosie's Excel sheet\n",
    "### Create a lists of observation tasks as letter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqL = ('A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B')\n",
    "seqM = ('B','G','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','G','C','F','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','G','G','E','E','E','E','D')\n",
    "\n",
    "print('HPSO LOW task distribution (number of occurences) A..B = (%.0f, %.0f)' % (seqL.count('A'), seqL.count('B')))\n",
    "tA = seqL.count('A') * hpso_durations['A']\n",
    "tB = seqL.count('B') * hpso_durations['B']\n",
    "print('HPSO LOW task distribution (observation time) A..B = (%.1f%%, %.1f%%)' % (100 * tA / (tA + tB), 100 * tB / (tA + tB)))\n",
    "\n",
    "tA = seqM.count('A')\n",
    "tB = seqM.count('B')\n",
    "tC = seqM.count('C')\n",
    "tD = seqM.count('D')\n",
    "tE = seqM.count('E')\n",
    "tF = seqM.count('F')\n",
    "tG = seqM.count('G')\n",
    "tt = len(seqM)\n",
    "\n",
    "print('\\nHPSO MID task distribution (number of occurences) A..G = (%.0f, %.0f, %.0f, %.0f, %.0f, %.0f, %.0f)' % \\\n",
    "      (tA, tB, tC, tD, tE, tF, tG))\n",
    "print('HPSO MID task distribution (observation time) A..G = (%.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%)' % \\\n",
    "      (100*tA/tt, 100*tB/tt, 100*tC/tt, 100*tD/tt, 100*tE/tt, 100*tF/tt, 100*tG/tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the lists of letters to build a lists of task objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtually execute the \"seqL\" task list for LOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = task_letters_to_objects(seqL, performance_dict)  # Set up the task list from the letter sequence\n",
    "\n",
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "# Run through the list of tasks, determining start and end times for their execution and the effect on the buffer\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary mapping wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "uids_completed = set()\n",
    "\n",
    "for task in tasks:\n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n",
    "\n",
    "buffer_evolution = collections.OrderedDict(sorted(buffer_deltas.items()))\n",
    "time_vals   = np.zeros(2 * len(buffer_evolution))\n",
    "buffer_vals = np.zeros(2 * len(buffer_evolution))\n",
    "\n",
    "i = 0\n",
    "buffer_val = 0\n",
    "time_val   = 0\n",
    "for k, delta in buffer_evolution.items(): \n",
    "    #print('(%.1f,\\t%.2f)' % (k/3600, delta))\n",
    "    time_val = k / 3600  # hours\n",
    "    time_vals[i]     = time_val\n",
    "    buffer_vals[i]   = buffer_val\n",
    "    buffer_val += delta  # Adds the buffer delta to the buffer's stored contents\n",
    "    time_vals[i+1]   = time_val  # we assume no time went by (writing being instantaneous)\n",
    "    buffer_vals[i+1] = buffer_val  # TeraBytes\n",
    "    i += 2\n",
    "\n",
    "plt.plot(time_vals, buffer_vals / 1e3, 'b-')\n",
    "plt.title('Evolution of the SDP Buffer while executing the supplied LOW sequence.\\nObservation time = %.1f hrs.' \n",
    "          ' Total execution time = %.1f hrs; Max buffer = %.1f PB' % (wall_clock / 3600, time_vals[-1],                                                                      np.max(buffer_vals)/1e3))\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('buffer usage (PB)')\n",
    "plt.xlim(0, time_vals[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(range(len(idle_time_durations))), idle_time_durations / 3600, marker='s', color = 'r', linewidth=0)\n",
    "plt.title('Idle time that tasks spend in the LOW buffer.\\nSummed idle time for all tasks = %.1f hrs.' % \n",
    "          (np.sum(idle_time_durations) / 3600))\n",
    "plt.xlabel('Task''s number in sequence')\n",
    "plt.ylabel('Time (hours)')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#               Table Row Title            Unit     Default?  Sum?   Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Req',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "hpso = hpso_lookup['A']  # hpso01.ICAL\n",
    "\n",
    "cfg = PipelineConfig(hpso=hpso)\n",
    "assert cfg.is_valid()\n",
    "tp = cfg.calc_tel_params()\n",
    "\n",
    "results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "print('Cache memory for hpso01.ICAL = %g TB' % results[1])\n",
    "print('Visibility rate for hpso01.ICAL = %g TB/s' % results[2])\n",
    "print('Rflop for hpso01.ICAL = %g PetaFLOPS' % results[3])\n",
    "\n",
    "# Another, slightly more roundabout, way to do the same as _compute_results \n",
    "# (tsnap_opt, nfacet_opt) = imp.find_optimal_Tsnap_Nfacet(tp)\n",
    "# result_expressions = iapi.get_result_expressions(results_map, tp)\n",
    "# results_for_pipeline = imp.evaluate_expressions(result_expressions, tp, tsnap_opt, nfacet_opt)\n",
    "# print(results_for_pipeline[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code taken from computing parametric model results by Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teles = (Telescopes.SKA1_Low, Telescopes.SKA1_Mid)\n",
    "bands = (Bands.Low, \n",
    "         Bands.Mid1, Bands.Mid2, Bands.Mid5A, Bands.Mid5B, Bands.Mid5C,\n",
    "         Bands.Sur1)\n",
    "parallel = 0  # Set this to 0 if PyMP is absent\n",
    "\n",
    "for pipeline in Pipelines.all:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % pipeline, teles, bands, [pipeline],\n",
    "                              parallel=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for band in bands:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % band, teles, [band], Pipelines.all,\n",
    "                              parallel = parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iapi.stack_bars_hpsos(\"HPSOs Computational Requirements [PetaFLOP/s]\", HPSOs.hpsos,\n",
    "                      parallel=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

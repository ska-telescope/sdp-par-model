{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.6.0 running Python 3.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "\n",
    "from sdp_par_model.scheduler import Definitions as sdefs\n",
    "from sdp_par_model.scheduler import Scheduler\n",
    "\n",
    "import scheduling.scheduling as sched\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "import bisect\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a sequence of tasks \n",
    "### (using letters A..G to define scheduling  blocks - see Google Drive or Python code  for Definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_capacity_low = 13.8  # PetaFlops\n",
    "flops_capacity_mid = 12.1  # PetaFlops\n",
    "\n",
    "cold_buffer_size_low = 30 # PetaBytes\n",
    "hot_buffer_size_low  = 20  # PetaBytes\n",
    "\n",
    "cold_buffer_size_mid = 30 # PetaBytes\n",
    "hot_buffer_size_mid  = 20  # PetaBytes\n",
    "\n",
    "\n",
    "seqL = ['B','A','A',] + ['B',]*32 + ['A',]*2 + ['B',]*73 + ['A',] + ['B',]*43\n",
    "seqM = ['B','G',] + ['B',]*34 + ['G','C','F',] + ['B',]*110 +['F',]*91 + ['G',]*2 + ['E',]*4 + ['D',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_simulate = seqL.copy()\n",
    "random.shuffle(sequence_to_simulate)  # Randomly shuffles the sequence of processing blocks (letters)\n",
    "keep_data_in_cold_buffer = False\n",
    "hpso_list = Scheduler.hpso_letters_to_hpsos(sequence_to_simulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "tel_str = \"LOW\"\n",
    "\n",
    "# THe block below saves the performance dictionary to file, but we won't use this for now (debugging)\n",
    "'''\n",
    "## Read  performace requirement lookup for all HPSOs. \n",
    "### If this lookup table does not exist, we create it, and save it to disk (to save time re-computing)\n",
    "performance_lookup_filename = \"performance_dict.data\"\n",
    "if os.path.isfile(performance_lookup_filename):\n",
    "    performance_dict = None\n",
    "    with open(performance_lookup_filename, \"rb\") as f:\n",
    "        performance_dict = pickle.load(f)\n",
    "else:\n",
    "    # Create a performance dictionary and write it to file\n",
    "    performance_dict = Scheduler.compute_performance_dictionary()\n",
    "    with open(performance_lookup_filename, \"wb\") as f:\n",
    "        pickle.dump(performance_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "'''\n",
    "\n",
    "performance_dictionary = Scheduler.compute_performance_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = Scheduler.hpsos_to_sdp_task_list(hpso_list, performance_dictionary, keep_data_in_cold_buffer)\n",
    "'''To show how the tasks are created, can print the sequence of Task objects.'''\n",
    "#for task in task_list:\n",
    "#    print(task)\n",
    "\n",
    "schedule = Scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                              assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "\n",
    "last_preservation_timestamp = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "max_t = last_preservation_timestamp\n",
    "print(\"SDP task sequence completes at t = %g hrs\" % (max_t / 3600))\n",
    "\n",
    "# Now we plot the results\n",
    "\n",
    "max_preservation = sorted(schedule.preserve_deltas.values())[-1]\n",
    "last_preservation_timestamp = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "xrange = [0, last_preservation_timestamp * 1.05]\n",
    "preserv_yrange = [0, max(max_preservation * 1.05, 1)]\n",
    "\n",
    "max_t = last_preservation_timestamp\n",
    "\n",
    "iapi.plot_deltas(schedule.flops_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP FLOP/s (capped at %.3g PetaFLOPS)' % (tel_str, flops_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaFLOP/s')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.memory_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Evolution of SDP working memory (RAM)', xlabel='wall clock time (hours)', ylabel='TeraByte')\n",
    "'''                 \n",
    "iapi.plot_deltas(schedule.cold_buffer_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP Cold buffer usage (capped at %.3g PetaByte)' % (tel_str, coldbuf_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.hot_buffer_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP Hot Buffer usage (capped at %.0f PetaByte)' % (tel_str, hotbuf_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.preserve_deltas, xrange=xrange, yrange=preserv_yrange, max_t=max_t, \n",
    "                 title='%s SDP Preservation usage (uncapped)' % tel_str, xlabel='wall clock time (hours)', ylabel='TeraByte')\n",
    "\n",
    "iapi.plot_deltas(schedule.ingest_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Ingest pipeline -> Cold Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')                 \n",
    "'''\n",
    "# Ingest -> Working memory pipeline is identical to Working Memory -> Cold Buffer (streaming)\n",
    "iapi.plot_deltas(schedule.mem_cold_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of Ingest working memory -> Cold Buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.cold_hot_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Cold Buffer -> Hot Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.hot_mem_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from hot buffer to working memory', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "iapi.plot_deltas(schedule.mem_hot_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from working memory to hot buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''                 \n",
    "iapi.plot_deltas(schedule.hot_preserve_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth %s (Hot Buffer -> Preservation)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dymanic generation, loosely based on Mark Ashdown's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Working directory = %s\" % os.getcwd())\n",
    "\n",
    "dtype_pmout = np.dtype([('name',   'U30'),\n",
    "                        ('tele',   'U10'),\n",
    "                        ('pipe',   'U10'),\n",
    "                        ('tobs',   'f8' ),\n",
    "                        ('tpoint', 'f8' ),\n",
    "                        ('texp',   'f8' ),\n",
    "                        ('rflop',  'f8' ),\n",
    "                        ('mvis',   'f8' ),\n",
    "                        ('mout',   'f8' )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = os.path.join('..','data','csv','2017-12-18-0d8d518_hpsos.csv')\n",
    "with open(ifile, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    line = next(reader)\n",
    "    pmout = np.zeros(len(line)-1, dtype=dtype_pmout)\n",
    "    \n",
    "    # Force non-defined entries to have NaN values\n",
    "    for entry in pmout:\n",
    "        entry[-1] = np.NaN\n",
    "        entry[-2] = np.NaN\n",
    "        entry[-3] = np.NaN\n",
    "        entry[-4] = np.NaN\n",
    "        entry[-5] = np.NaN\n",
    "        entry[-6] = np.NaN    \n",
    "        \n",
    "    # Read all the column names, discarding everything from the first space onwards\n",
    "    pmout['name'] = [x.split()[0] for x in line[1:]]  \n",
    "    print('Column headers:\\n')\n",
    "    print(line)\n",
    "    print('\\nExtracted HPSO names:\\n')\n",
    "    print(pmout['name'])\n",
    "    \n",
    "    # Now iterate across all rows. If a row name is one we're interested in, read the values for all columns\n",
    "    # and write them to the pmout data structure. Skip empty or unrecognized rows\n",
    "    for line in reader:  \n",
    "        if len(line) == 0:\n",
    "            pass  # empty line; skip\n",
    "        elif line[0] == 'Telescope':\n",
    "            pmout['tele'] = line[1:]\n",
    "        elif line[0] == 'Pipeline':\n",
    "            pmout['pipe'] = line[1:]\n",
    "        elif line[0] == 'Observation Time [s]':\n",
    "            pmout['tobs'] = line[1:]\n",
    "        elif line[0] == 'Pointing Time [s]':\n",
    "            pmout['tpoint'] = line[1:]\n",
    "        elif line[0] == 'Total Time [s]':\n",
    "            pmout['texp'] = line[1:]\n",
    "        elif line[0] == 'Total Compute Requirement [PetaFLOP/s]':\n",
    "            pmout['rflop'] = line[1:]\n",
    "        elif line[0] == 'Visibility Buffer [PetaBytes]':\n",
    "            pmout['mvis'] = line[1:]\n",
    "        elif line[0] == 'Output size [TB]':\n",
    "            pmout['mout'] = line[1:]\n",
    "        else:\n",
    "            pass  # row title is not in the list of ones we're interested in\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pmout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_to_hpsos = {}\n",
    "hpso_params = {}\n",
    "\n",
    "#ifile = os.path.join('..','data','csv','2018-09-17-2c761d6_hpsos.csv')\n",
    "ifile = os.path.join('..','data','csv','2017-12-18-0d8d518_hpsos.csv')\n",
    "oform = 'hpsos_{t}.txt'\n",
    "\n",
    "tele = [('low', 'SKA1_Low'),\n",
    "        ('mid', 'SKA1_Mid')]\n",
    "\n",
    "# Loop over telescopes.\n",
    "\n",
    "for tel, tinp in tele:\n",
    "    ofile = oform.format(t=tel)\n",
    "\n",
    "    # Extract project information.\n",
    "    proj = sched.extract_projects(ifile, tinp)\n",
    "\n",
    "    # Write list of projects.\n",
    "    sched.write_projects(ofile, proj)\n",
    "    print(\"Processed %s, %s\" % (tel, tinp))\n",
    "    break\n",
    "\n",
    "'''\n",
    "for hpso in HPSOs.hpsos_original:\n",
    "    p = ParameterContainer()\n",
    "    hpso_params = apply_hpso_parameters(p, hpso, HPSOs.hpso_tasks[hpso][0])\n",
    "    tel = hpso_params.telescope\n",
    "    texp = hpso_params.Texp\n",
    "    tpoint = hpso_params.Tpoint\n",
    "    \n",
    "    if not tel in tel_to_hpsos:\n",
    "        tel_to_hpsos[tel] = set()\n",
    "    tel_to_hpsos[tel].add(hpso)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hpso_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_to_hpsos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This can also be read from e.g. a CSV file if you want flexible schedules to be simulated\n",
    "#### Using a variant of Mark Ashdown's schedule generating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = os.path.join('..', 'scheduling', 'hpsos.csv')\n",
    "\n",
    "projects = {}\n",
    "\n",
    "# Loop over telescopes.\n",
    "for tinp in ('SKA1_Low', 'SKA1_Mid'):\n",
    "    proj = sched.extract_projects(ifile, tinp)\n",
    "    projects[tinp] = proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the length of a scheduling block and the length of the sequence\n",
    "# to generate.\n",
    "\n",
    "tsched = 6.0 * 3600.0\n",
    "tseq = 10.0 * 24.0 * 3600.0\n",
    "allow_short_tobs = False\n",
    "generated_sequences = {}\n",
    "\n",
    "for t in ('SKA1_Low', 'SKA1_Mid'):\n",
    "    # Generate sequence of observations.\n",
    "    proj = projects[t]\n",
    "    seq = sched.generate_sequence(proj, tsched, tseq,\n",
    "                                  allow_short_tobs=allow_short_tobs)\n",
    "    sequence = []\n",
    "    for item in seq:\n",
    "        sequence.append(item[1])  # the HPSO identifier\n",
    "\n",
    "    generated_sequences[t] = sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code follows below - may not work since updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old-school letter sequence method:\n",
    "## Create the Scheduler object, sets it up, execute the scheduler, and plot the results\n",
    "At the moment the Scheduler object contains the functionality of the SDP simulator, the scheduling code itself, as well as the generated schedule objects. In future we may wish to split them into separate classes and objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a number of randomized sequences, looking at spread of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_of_runs = 100\n",
    "\n",
    "sequence_to_simulate = seqL\n",
    "\n",
    "# Caps are all in \"Peta\" units (FLOPS, or Bytes)\n",
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "tel_str = \"LOW\"\n",
    "\n",
    "## Read  performace requirement lookup for all HPSOs. \n",
    "### If this lookup table does not exist, we create it, and save it to disk (to save time re-computing)\n",
    "performance_lookup_filename = \"performance_dict.data\"\n",
    "performance_dict = None\n",
    "if os.path.isfile(performance_lookup_filename):\n",
    "    performance_dict = None\n",
    "    with open(performance_lookup_filename, \"rb\") as f:\n",
    "        performance_dict = pickle.load(f)\n",
    "else:\n",
    "    raise Exception(\"No Performance dictionary file found!\")\n",
    "\n",
    "runtimes = np.zeros(nr_of_runs)\n",
    "for i in range(nr_of_runs):\n",
    "    # Randomly shuffles the sequence of processing blocks (letters)\n",
    "    random.shuffle(sequence_to_simulate)\n",
    "    sdp_scheduler = Scheduler()  # Create a new scheduler, because the Scheduler has a state\n",
    "    sdp_scheduler.set_performance_dictionary(performance_dict)\n",
    "    task_list = sdp_scheduler.hpso_letters_to_sdp_task_list(sequence_to_simulate)\n",
    "\n",
    "    schedule = sdp_scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                                      assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "    max_t = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "    runtimes[i] = max_t\n",
    "    print(\"Run %d of %d : SDP task seq completed at t = %g hrs\" % (i+1, nr_of_runs, (max_t / 3600)))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(runtimes/3600)\n",
    "plt.title('Distribution of execution times (median = %.1f hours)' % np.median(runtimes/3600), Fontsize=20)\n",
    "plt.xlabel('Hours', Fontsize=16)\n",
    "plt.ylabel('Nr of occurrences', Fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to approximately replicate Rosie's results. Check (rerun) the notebook at that repository revision to regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

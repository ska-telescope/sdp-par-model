{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.7.2 running Python 3.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "\n",
    "from sdp_par_model.scheduler import Definitions as sdefs\n",
    "from sdp_par_model.scheduler import Scheduler\n",
    "\n",
    "import scheduling.scheduling as sched\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "import bisect\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load precomputed performance dictionary (or compute it anew and save to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "performance_lookup_filename = os.path.join(\"..\", \"performance_dict.data\")\n",
    "if os.path.isfile(performance_lookup_filename):\n",
    "    performance_dict = None\n",
    "    with open(performance_lookup_filename, \"rb\") as f:\n",
    "        performance_dict = pickle.load(f)\n",
    "else:\n",
    "    # Read latest CSV file\n",
    "    results = iapi.read_csv(iapi.newest_csv(iapi.find_csvs(), \"hpsos\"))\n",
    "    # Create a performance dictionary and write it to file\n",
    "    performance_dict = Scheduler.compute_performance_dictionary(results)\n",
    "    with open(performance_lookup_filename, \"wb\") as f:\n",
    "        pickle.dump(performance_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a sequence of HPSOs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_capacity_low = 13.8  # PetaFlops\n",
    "flops_capacity_mid = 12.1  # PetaFlops\n",
    "\n",
    "cold_buffer_size_low = 30 * (c.peta / c.tera) # TeraBytes\n",
    "hot_buffer_size_low  = 30 * (c.peta / c.tera) # TeraBytes\n",
    "\n",
    "cold_buffer_size_mid = 30 * (c.peta / c.tera) # TeraBytes\n",
    "hot_buffer_size_mid  = 30 * (c.peta / c.tera) # TeraBytes\n",
    "\n",
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "tel_str = \"LOW\"\n",
    "keep_data_in_cold_buffer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: using letters A..G to define scheduling  blocks - see Google Drive or Python code  for Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqL = ['B','A','A',] + ['B',]*32 + ['A',]*2 + ['B',]*73 + ['A',] + ['B',]*43\n",
    "seqM = ['B','G',] + ['B',]*34 + ['G','C','F',] + ['B',]*110 +['F',]*91 + ['G',]*2 + ['E',]*4 + ['D',]\n",
    "\n",
    "sequence_to_simulate = seqL.copy()\n",
    "random.shuffle(sequence_to_simulate)  # Randomly shuffles the sequence of processing blocks (letters)\n",
    "keep_data_in_cold_buffer = False\n",
    "hpso_list = Scheduler.hpso_letters_to_hpsos(sequence_to_simulate)\n",
    "t_obs_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or Option 2: using dymanic generation, loosely based on Mark Ashdown's scheduling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create sets of hpsos from which to build the sequences\n",
    "hpsos_low = {HPSOs.hpso01, HPSOs.hpso02a, HPSOs.hpso02b}\n",
    "hpsos_mid = {HPSOs.hpso13 , HPSOs.hpso15, HPSOs.hpso22, HPSOs.hpso27and33, HPSOs.hpso32, HPSOs.hpso37a, HPSOs.hpso37b, HPSOs.hpso37c,\n",
    "             HPSOs.hpso38a, HPSOs.hpso38a}\n",
    "\n",
    "hpso_set = hpsos_low\n",
    "dt_block = 6.0 * 3600.0        # duration of each scheduling block, in seconds\n",
    "dt_seq = 10.0 * 24.0 * 3600.0  # duration floor of the entire sequence, in seconds\n",
    "allow_short_tobs = False\n",
    "\n",
    "\n",
    "(hpso_list, t_obs_list) = Scheduler.generate_sequence(hpso_set, performance_dict, dt_block, dt_seq, allow_short_tobs)\n",
    "\n",
    "ttotal = np.sum(t_obs_list)\n",
    "print('Generated a list containing %d HPSOs, representing a cumulative observation time of %g hours.' \n",
    "      % (len(hpso_list), ttotal / 3600))\n",
    "print('Cum. observation time is %g %% of the desired value' % (100 * ttotal / dt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In either case, use the HPSOs list to generate a schedule and run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "task_list = Scheduler.hpsos_to_sdp_task_list(hpso_list, performance_dict, t_obs_list, keep_data_in_cold_buffer)\n",
    "'''To show how the tasks are created, can print the sequence of Task objects.'''\n",
    "#for task in task_list:\n",
    "#    print(task)\n",
    "\n",
    "schedule = Scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                              assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "\n",
    "\n",
    "# Now we plot the results\n",
    "\n",
    "last_preservation_timestamp = sorted(schedule.hot_preserve_pipe_delta.keys())[-1]\n",
    "max_t = last_preservation_timestamp\n",
    "print(\"SDP task sequence completes at t = %g hrs\" % (max_t / 3600))\n",
    "xrange = [0, max_t * 1.05]\n",
    "\n",
    "iapi.plot_deltas(schedule.flops_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='%s SDP FLOP/s (capped at %.3g PetaFLOPS)' % (tel_str, flops_cap), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaFLOP/s')\n",
    "\n",
    "iapi.plot_deltas(schedule.memory_deltas, xrange=xrange, max_t=max_t, factor=1e3,\n",
    "                 title='Evolution of SDP working memory (RAM)', xlabel='wall clock time (hours)', ylabel='GigaByte')\n",
    "                 \n",
    "iapi.plot_deltas(schedule.cold_buffer_deltas, xrange=xrange, max_t=max_t, factor=1e-3, \n",
    "                 title='%s SDP Cold buffer usage (capped at %.3g PetaByte)' % (tel_str, coldbuf_cap * 1e-3), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.hot_buffer_deltas, xrange=xrange, max_t=max_t, factor=1e-3, \n",
    "                 title='%s SDP Hot Buffer usage (capped at %.0f PetaByte)' % (tel_str, hotbuf_cap * 1e-3), \n",
    "                 xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "iapi.plot_deltas(schedule.preserve_deltas, xrange=xrange, max_t=max_t, factor=1e-3, \n",
    "                 title='%s SDP Preservation usage (uncapped)' % tel_str, xlabel='wall clock time (hours)', ylabel='PetaByte')\n",
    "\n",
    "iapi.plot_deltas(schedule.ingest_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Ingest pipeline -> Cold Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')                 \n",
    "'''\n",
    "# Ingest -> Working memory pipeline is identical to Working Memory -> Cold Buffer (streaming)\n",
    "iapi.plot_deltas(schedule.mem_cold_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of Ingest working memory -> Cold Buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.cold_hot_pipe_deltas, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth of %s (Cold Buffer -> Hot Buffer)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''\n",
    "iapi.plot_deltas(schedule.hot_mem_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from hot buffer to working memory', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "iapi.plot_deltas(schedule.mem_hot_pipe_delta, xrange=xrange, max_t=max_t, \n",
    "                 title='Bandwidth usage of pipeline from working memory to hot buffer', \n",
    "                 xlabel='wall clock time (hours)', ylabel='TeraByte/s', colour='c')\n",
    "'''                 \n",
    "iapi.plot_deltas(schedule.hot_preserve_pipe_delta, xrange=xrange, max_t=max_t, factor=1e3,\n",
    "                 title='Bandwidth %s (Hot Buffer -> Preservation)' % tel_str, \n",
    "                 xlabel='wall clock time (hours)', ylabel='GigaByte/s', colour='c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a number of randomized sequences, looking at spread of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, create sets of hpsos from which to build the sequences\n",
    "hpsos_low = {HPSOs.hpso01, HPSOs.hpso02a, HPSOs.hpso02b}\n",
    "hpsos_mid = {HPSOs.hpso13 , HPSOs.hpso15, HPSOs.hpso22, HPSOs.hpso27and33, HPSOs.hpso32, HPSOs.hpso37a, HPSOs.hpso37b, HPSOs.hpso37c,\n",
    "             HPSOs.hpso38a, HPSOs.hpso38a}\n",
    "\n",
    "nr_of_runs = 100\n",
    "hpso_set = hpsos_low\n",
    "dt_block = 6.0 * 3600.0        # duration of each scheduling block, in seconds\n",
    "dt_seq = 10.0 * 24.0 * 3600.0  # duration floor of the entire sequence, in seconds\n",
    "allow_short_tobs = False\n",
    "keep_data_in_cold_buffer = False\n",
    "\n",
    "# Caps are all in \"Peta\" units (FLOPS, or Bytes)\n",
    "flops_cap   = flops_capacity_low\n",
    "coldbuf_cap = cold_buffer_size_low\n",
    "hotbuf_cap  = hot_buffer_size_low\n",
    "\n",
    "\n",
    "runtimes = np.zeros(nr_of_runs)\n",
    "for i in range(nr_of_runs):\n",
    "    # Generate random sequence\n",
    "    (hpso_list, t_obs_list) = Scheduler.generate_sequence(hpso_set, performance_dict, dt_block, dt_seq, allow_short_tobs)\n",
    "\n",
    "    task_list = Scheduler.hpsos_to_sdp_task_list(hpso_list, performance_dict, t_obs_list, keep_data_in_cold_buffer)\n",
    "    '''To show how the tasks are created, can print the sequence of Task objects.'''\n",
    "    #for task in task_list:\n",
    "    #    print(task)\n",
    "\n",
    "    schedule = Scheduler.schedule(task_list, flops_cap, hotbuf_cap, coldbuf_cap,  \n",
    "                                  assign_flops_fraction=0.5, assign_bw_fraction=0.5, max_nr_iterations=1000)\n",
    "\n",
    "    max_t = sorted(schedule.preserve_deltas.keys())[-1]\n",
    "    runtimes[i] = max_t\n",
    "    print(\"Run %d of %d : SDP task seq completed at t = %g hrs\" % (i+1, nr_of_runs, (max_t / 3600)))\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "plt.hist(runtimes/3600)\n",
    "plt.title('Distribution of execution times (median = %.1f hours)' % np.median(runtimes/3600), Fontsize=20)\n",
    "plt.xlabel('Hours', Fontsize=16)\n",
    "plt.ylabel('Nr of occurrences', Fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to approximately replicate Rosie's results. Check (rerun) the notebook at that repository revision to regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.0.0 running Python 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model import evaluate as imp\n",
    "from sdp_par_model.config import PipelineConfig\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "import numpy as np\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful structures and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needs some refactoring methinks; idea would be to specify HPSOs instead of \"letters\". \n",
    "hpso_lookup = {'A' : HPSOs.hpso01, \n",
    "               'B' : HPSOs.hpso04c,  # TODO: This task not properly defined yet\n",
    "               'C' : HPSOs.hpso13, \n",
    "               'D' : HPSOs.hpso14,\n",
    "               'E' : HPSOs.hpso15,\n",
    "               'F' : HPSOs.hpso27,\n",
    "               'G' : HPSOs.hpso37c}\n",
    "\n",
    "# The following results map was copied from examples used by Peter Wortmann. It defines values we wish to calculate.\n",
    "#               Title                      Unit       Default? Sum?             Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Rate',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "\n",
    "class SDPTask:\n",
    "    uid          = None  # Optional: unique ID; can be used for sequencing\n",
    "    t_min_start  = None  # Earliest wall clock time that this task can / may start (in seconds)\n",
    "    prec_task    = None  # Preceding task (uid) that needs to complete before this one can start\n",
    "    t_fixed      = None  # fixed minimum duration of this task (e.g. for an observation)\n",
    "    flopcount    = None  # Number of floating point operations required to complete this task\n",
    "    data_in      = None  # Amount of data (in TB) that this task needs for input (usually read from the hot buffer)    \n",
    "    data_out     = None  # Amount of data (in TB) that this task outputs (usually written to hot buffer)    \n",
    "\n",
    "def add_delta(deltas, t, delta):\n",
    "    \"\"\"\n",
    "    Adds a {t : delta} pair to a timestamped dictionary that maps timestamps to delta values.\n",
    "    If the supplied t already maps to a value, the supplied delta is added\n",
    "    \"\"\"\n",
    "    if t in deltas:\n",
    "        warnings.warn('Timestamp entry already exists in the timeline')\n",
    "        deltas[t] += delta\n",
    "    else:\n",
    "        deltas[t] = delta\n",
    "        \n",
    "def task_letters_to_objects(letter_sequence, performance_dict):\n",
    "    \"\"\"\n",
    "    Converts a list of task letters into a sequence of task objects\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    uid =  -1\n",
    "    for task_letter in letter_sequence:\n",
    "        hpso = hpso_lookup[task_letter]\n",
    "        \n",
    "        for subtask in HPSOs.hpso_subtasks[hpso]:\n",
    "            uid += 1\n",
    "            t = SDPTask()\n",
    "            t.uid = uid\n",
    "\n",
    "            p = ParameterContainer()\n",
    "            apply_hpso_parameters(p, hpso=hpso, hpso_subtask=subtask)\n",
    "            \n",
    "            # The following way of linking tasks sequentually is very crude; must be improved\n",
    "            if subtask in HPSOs.ingest_subtasks:\n",
    "                t.t_obs = performance_dict[hpso]['Tobs']\n",
    "                t.bufsize = t.t_obs * performance_dict[hpso][subtask]['ingestRate']\n",
    "            else:  # has to happen after previous task. may not be the case for the DPrep tasks?\n",
    "                t.prec_task = uid - 1 \n",
    "                t.t_obs = 0\n",
    "                t.bufsize = 0\n",
    "\n",
    "            t.flopcount = performance_dict[hpso][subtask]['compRate'] * performance_dict[hpso]['Tobs']\n",
    "            tasks.append(t)\n",
    "    return tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes parametric model-computed performace requirements for each HPSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance_dict = {}  # A dictionary of dictionaries.  HPSO requirements are computed once and stored as lookups\n",
    "\n",
    "# As a test we loop over all HPSOs we wish to handle, computing results for each\n",
    "for task_letter in sorted(hpso_lookup.keys()):\n",
    "    hpso = hpso_lookup[task_letter]\n",
    "    print('*** Processing task type %s => %s ***\\n' % (task_letter, hpso))\n",
    "    if not hpso in performance_dict:\n",
    "        performance_dict[hpso] = {}\n",
    "        \n",
    "    for subtask in HPSOs.hpso_subtasks[hpso]:\n",
    "        print('subtask -> %s' % subtask)\n",
    "        if not subtask in performance_dict[hpso]:\n",
    "            performance_dict[hpso][subtask] = {}\n",
    "        \n",
    "        cfg = PipelineConfig(hpso=hpso, hpso_subtask=subtask)\n",
    "        (valid, msgs) = cfg.is_valid()\n",
    "        if not valid:\n",
    "            print(\"Invalid configuration!\")\n",
    "            for msg in msgs:\n",
    "                print(msg)\n",
    "            raise AssertionError(\"Invalid config\")\n",
    "        tp = cfg.calc_tel_params()\n",
    "        results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "        \n",
    "        performance_dict[hpso]['Tobs'] = tp.Tobs  # Observation time\n",
    "        performance_dict[hpso][subtask]['ingestRate'] = results[0]\n",
    "        performance_dict[hpso][subtask]['cache'] = results[1]\n",
    "        performance_dict[hpso][subtask]['visRate'] = results[2]\n",
    "        performance_dict[hpso][subtask]['compRate'] = results[3]\n",
    "        \n",
    "        print('Buffer ingest rate\\t= %g TB/s' % results[0])\n",
    "        print('Cache memory\\t= %g TB' % results[1])\n",
    "        print('Visibility IO rate\\t= %g TB/s' % results[2])\n",
    "        print('Compute Rate\\t= %g PetaFLOP/s' % results[3])\n",
    "        print()\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to replicate Rosie's results. Check repo regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours -- TODO check whether correct\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of \"Low\" and \"Mid\" sequences from Rosie's Excel sheet\n",
    "### Create a lists of observation tasks as letter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqL = ('A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B')\n",
    "seqM = ('B','G','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','G','C','F','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','G','G','E','E','E','E','D')\n",
    "\n",
    "print('HPSO LOW task distribution (number of occurences) A..B = (%.0f, %.0f)' % (seqL.count('A'), seqL.count('B')))\n",
    "tA = seqL.count('A') * hpso_durations['A']\n",
    "tB = seqL.count('B') * hpso_durations['B']\n",
    "print('HPSO LOW task distribution (observation time) A..B = (%.1f%%, %.1f%%)' % (100 * tA / (tA + tB), 100 * tB / (tA + tB)))\n",
    "\n",
    "tA = seqM.count('A')\n",
    "tB = seqM.count('B')\n",
    "tC = seqM.count('C')\n",
    "tD = seqM.count('D')\n",
    "tE = seqM.count('E')\n",
    "tF = seqM.count('F')\n",
    "tG = seqM.count('G')\n",
    "tt = len(seqM)\n",
    "\n",
    "print('\\nHPSO MID task distribution (number of occurences) A..G = (%.0f, %.0f, %.0f, %.0f, %.0f, %.0f, %.0f)' % \\\n",
    "      (tA, tB, tC, tD, tE, tF, tG))\n",
    "print('HPSO MID task distribution (observation time) A..G = (%.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%)' % \\\n",
    "      (100*tA/tt, 100*tB/tt, 100*tC/tt, 100*tD/tt, 100*tE/tt, 100*tF/tt, 100*tG/tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the lists of letters to build a lists of task objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtually execute the \"seqL\" task list for LOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = task_letters_to_objects(seqL, performance_dict)  # Set up the task list from the letter sequence\n",
    "\n",
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "# Run through the list of tasks, determining start and end times for their execution and the effect on the buffer\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary mapping wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "uids_completed = set()\n",
    "\n",
    "for task in tasks:\n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n",
    "\n",
    "buffer_evolution = collections.OrderedDict(sorted(buffer_deltas.items()))\n",
    "time_vals   = np.zeros(2 * len(buffer_evolution))\n",
    "buffer_vals = np.zeros(2 * len(buffer_evolution))\n",
    "\n",
    "i = 0\n",
    "buffer_val = 0\n",
    "time_val   = 0\n",
    "for k, delta in buffer_evolution.items(): \n",
    "    #print('(%.1f,\\t%.2f)' % (k/3600, delta))\n",
    "    time_val = k / 3600  # hours\n",
    "    time_vals[i]     = time_val\n",
    "    buffer_vals[i]   = buffer_val\n",
    "    buffer_val += delta  # Adds the buffer delta to the buffer's stored contents\n",
    "    time_vals[i+1]   = time_val  # we assume no time went by (writing being instantaneous)\n",
    "    buffer_vals[i+1] = buffer_val  # TeraBytes\n",
    "    i += 2\n",
    "\n",
    "plt.plot(time_vals, buffer_vals / 1e3, 'b-')\n",
    "plt.title('Evolution of the SDP Buffer while executing the supplied LOW sequence.\\nObservation time = %.1f hrs.' \n",
    "          ' Total execution time = %.1f hrs; Max buffer = %.1f PB' % (wall_clock / 3600, time_vals[-1],                                                                      np.max(buffer_vals)/1e3))\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('buffer usage (PB)')\n",
    "plt.xlim(0, time_vals[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(range(len(idle_time_durations))), idle_time_durations / 3600, marker='s', color = 'r', linewidth=0)\n",
    "plt.title('Idle time that tasks spend in the LOW buffer.\\nSummed idle time for all tasks = %.1f hrs.' % \n",
    "          (np.sum(idle_time_durations) / 3600))\n",
    "plt.xlabel('Task''s number in sequence')\n",
    "plt.ylabel('Time (hours)')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#               Table Row Title            Unit     Default?  Sum?   Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Req',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "hpso = hpso_lookup['A']  # hpso01.ICAL\n",
    "\n",
    "cfg = PipelineConfig(hpso=hpso)\n",
    "assert cfg.is_valid()\n",
    "tp = cfg.calc_tel_params()\n",
    "\n",
    "results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "print('Cache memory for hpso01.ICAL = %g TB' % results[1])\n",
    "print('Visibility rate for hpso01.ICAL = %g TB/s' % results[2])\n",
    "print('Rflop for hpso01.ICAL = %g PetaFLOPS' % results[3])\n",
    "\n",
    "# Another, slightly more roundabout, way to do the same as _compute_results \n",
    "# (tsnap_opt, nfacet_opt) = imp.find_optimal_Tsnap_Nfacet(tp)\n",
    "# result_expressions = iapi.get_result_expressions(results_map, tp)\n",
    "# results_for_pipeline = imp.evaluate_expressions(result_expressions, tp, tsnap_opt, nfacet_opt)\n",
    "# print(results_for_pipeline[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code taken from computing parametric model results by Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teles = (Telescopes.SKA1_Low, Telescopes.SKA1_Mid)\n",
    "bands = (Bands.Low, \n",
    "         Bands.Mid1, Bands.Mid2, Bands.Mid5A, Bands.Mid5B, Bands.Mid5C,\n",
    "         Bands.Sur1)\n",
    "parallel = 0  # Set this to 0 if PyMP is absent\n",
    "\n",
    "for pipeline in Pipelines.all:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % pipeline, teles, bands, [pipeline],\n",
    "                              parallel=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for band in bands:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % band, teles, [band], Pipelines.all,\n",
    "                              parallel = parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iapi.stack_bars_hpsos(\"HPSOs Computational Requirements [PetaFLOP/s]\", HPSOs.hpsos,\n",
    "                      parallel=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

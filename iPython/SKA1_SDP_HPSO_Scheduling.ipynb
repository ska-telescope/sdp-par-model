{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP HPSO Scheduling\n",
    "\n",
    "Last run with Jupyter Notebook 5.0.0 running Python 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path += ['..']\n",
    "from sdp_par_model import reports as iapi\n",
    "from sdp_par_model import evaluate as imp\n",
    "from sdp_par_model.config import PipelineConfig\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "import numpy as np\n",
    "import collections\n",
    "import warnings\n",
    "import bisect\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define structures and methods for handling SDP tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-12  # Used for numerical stability (rounding errors)\n",
    "\n",
    "# Needs some refactoring methinks; idea would be to specify HPSOs instead of \"letters\". \n",
    "hpso_lookup = {'A' : HPSOs.hpso01, \n",
    "               'B' : HPSOs.hpso04c,  # TODO: This task non-imaging. Interesting use case.\n",
    "               'C' : HPSOs.hpso13, \n",
    "               'D' : HPSOs.hpso14,\n",
    "               'E' : HPSOs.hpso15,\n",
    "               'F' : HPSOs.hpso27,\n",
    "               'G' : HPSOs.hpso37c}\n",
    "\n",
    "# The following results map was copied from examples used by Peter Wortmann. It defines values we wish to calculate.\n",
    "#               Title                      Unit       Default? Sum?             Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Rate',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "\n",
    "class DataLocation:\n",
    "    ingest_buffer = \"ingestbuffer\"\n",
    "    cold_buffer   = \"coldbuffer\"\n",
    "    hot_buffer    = \"hotbuffer\"\n",
    "    preserve      = \"preserve\"\n",
    "\n",
    "class SDPTask:\n",
    "    uid         = None  # Unique ID - must be defined at task creation. Used for hashing and equality.\n",
    "    description = None  # A human-readable description of the task\n",
    "    t_min_start = None  # Earliest wall clock time (in seconds) that this task can / may start\n",
    "    prec_task   = None  # Preceding task that needs to complete before this one can start (can be None)\n",
    "    t_fixed     = None  # fixed minimum duration (in seconds) of this task (e.g. for an observation)\n",
    "    data_in     = None  # Amount of data (in TB) this task requires to read from (presumably hot) buffer *before* starting\n",
    "    memsize     = None  # The amount of SDP working memory (in TB) needed to perform this task\n",
    "    flopcount   = None  # Number of operations (in PetaFLOP) required to complete this task\n",
    "    data_out    = None  # Amount of data (in TB) this task stores *after* finishing (written to buffer or long-term preserve)\n",
    "    data_source = None  # Data Location\n",
    "    data_target = None  # Data Location\n",
    "\n",
    "    def __init__(self, unique_id, description=None):\n",
    "        self.uid = unique_id\n",
    "        self.description = description\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.uid)  # Required for using Task objects in sets.\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, SDPTask):\n",
    "            return False\n",
    "        return (self.uid == other.uid)\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"SDPTask #%d (type undefined): \" % self.uid\n",
    "        if self.description is not None:\n",
    "            s = \"SDPTask #%d (%s):\" % (self.uid, self.description)\n",
    "        fields = self.__dict__\n",
    "        for key_string in fields.keys():\n",
    "            if key_string == 'uid':\n",
    "                continue  # We already printed the uid\n",
    "            elif (key_string == 'prec_task') and (isinstance(fields[key_string], SDPTask)):\n",
    "                value_string = \"SDPTask #%d\" % fields[key_string].uid  # Prevent recursion\n",
    "            else:\n",
    "                value_string = str(fields[key_string])\n",
    "                \n",
    "            if len(value_string) > 40:\n",
    "                value_string = value_string[:40] + \"... (truncated)\"\n",
    "            s += \"\\n %s\\t\\t= %s\" % (key_string, value_string)\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def set_param(self, param_name, value, prevent_overwrite=True, require_overwrite=False):\n",
    "        \"\"\"\n",
    "        Method for setting a parameter in a safe way. By default first checks that the value has not already been defined.\n",
    "        Useful for preventing situations where values may inadvertently be overwritten.\n",
    "\n",
    "        :param param_name: The name of the parameter/field that needs to be assigned - provided as text\n",
    "        :param value: the value to be written (as actual data type, i.e. not necessarily text) \n",
    "        :param prevent_overwrite: Disallows this value to be overwritten once defined. Default = True.\n",
    "        :param require_overwrite: Only allows value to be changed if it already exists. Default = False.\n",
    "        \"\"\"\n",
    "        assert isinstance(param_name, str)\n",
    "        if prevent_overwrite:\n",
    "            if require_overwrite:\n",
    "                raise AssertionError(\"Cannot simultaneously require and prevent overwrite of parameter '%s'\" % param_name)\n",
    "\n",
    "            if hasattr(self, param_name):\n",
    "                if eval('self.%s == value' % param_name):\n",
    "                    print('reassigning value for %s = %s' % (param_name, str(value)))  #TODO remove\n",
    "                    warnings.warn('Inefficiency : reassigning parameter \"%s\" with same value as before.' % param_name)\n",
    "                else:\n",
    "                    try:\n",
    "                        assert eval('self.%s == None' % param_name)\n",
    "                    except AssertionError:\n",
    "                        raise AssertionError(\"The parameter %s has already been defined and may not be overwritten.\" % param_name)\n",
    "\n",
    "        elif require_overwrite and (not hasattr(self, param_name)):\n",
    "            raise AssertionError(\"Parameter '%s' is undefined and therefore cannot be assigned\" % param_name)\n",
    "\n",
    "        exec('self.%s = value' % param_name)  # Write the value\n",
    "\n",
    "            \n",
    "def task_letters_to_SDPTask_list(letter_sequence, performance_dict):\n",
    "    \"\"\"\n",
    "    Converts a list of task letters into a list of SDPTask objects\n",
    "    @param letter_sequence : a sequence of HPSOs, defined by Rosie's A..G lettering scheme. TODO: replace by actual HPSO names\n",
    "    @param performance_dict : a dictionary of computational requirements for each HPSO; these need to be computed only once\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    uid =  -1\n",
    "    prev_ingest_task = None\n",
    "    for task_letter in letter_sequence:\n",
    "        hpso = hpso_lookup[task_letter]\n",
    "        hpso_subtasks = HPSOs.hpso_subtasks[hpso]\n",
    "        nr_subtasks = len(hpso_subtasks)\n",
    "        \n",
    "        assert nr_subtasks >= 2  # We assume that the tast as *at least* an Ingest and an RCal component\n",
    "        if not (hpso_subtasks[0] in HPSOs.ingest_subtasks) and (hpso_subtasks[1] in HPSOs.rcal_subtasks):\n",
    "            # this is assumed true for all HPSOs - hence raising an assertion error if not\n",
    "            raise AssertionError(\"Assumption was wrong - some HPSO apparently doesn't involve Ingest + RCal\")\n",
    "        \n",
    "        # Ingest and Rcal are combined into a a single task object, as they cannot be separated         \n",
    "        uid += 1  # the unique id of the combined Ingest+Rcal task        \n",
    "        t = SDPTask(uid, 'Ingest+RCal')\n",
    "        if prev_ingest_task is not None:\n",
    "            t.set_param(\"prec_task\", prev_ingest_task) # previous ingest task needs to be complete before this one can start\n",
    "        t.set_param(\"t_fixed\", performance_dict[hpso]['Tobs'])\n",
    "        t.set_param(\"data_source\", DataLocation.ingest_buffer)\n",
    "        t.set_param(\"data_target\", DataLocation.cold_buffer)\n",
    "        prev_ingest_task = t  # current (ingest+rcal) task remembered for later; subtasks may only start once this completes\n",
    "        \n",
    "        # Ingest\n",
    "        subtask = hpso_subtasks[0]\n",
    "        data_in = 0 # data is acquired in real time. TODO: How much?\n",
    "        memsize = performance_dict[hpso][subtask]['cache']\n",
    "        flopcount = t.t_fixed * performance_dict[hpso][subtask]['compRate']\n",
    "        data_out = t.t_fixed * performance_dict[hpso][subtask]['ingestRate']  # ingested data to [cold] buffer.\n",
    "        \n",
    "        # RCal\n",
    "        subtask = hpso_subtasks[1]  \n",
    "        data_in += 0 # data is acquired in real time. TODO: How much?\n",
    "        memsize += performance_dict[hpso][subtask]['cache']\n",
    "        flopcount += t.t_fixed * performance_dict[hpso][subtask]['compRate']\n",
    "        data_out  += 0 # What output does RCal generate? Just set it to zero for now. Are ingestRate and visRate relevant?\n",
    "        \n",
    "        # Set the aggregated task parameters that were computed\n",
    "        t.set_param(\"data_in\", data_in)\n",
    "        t.set_param(\"memsize\", memsize)\n",
    "        t.set_param(\"flopcount\", flopcount)\n",
    "        t.set_param(\"data_out\", data_out)   \n",
    "\n",
    "        tasks.append(t)\n",
    "        ingest_rcal_data = data_out  # This is the data generated by the combined Ingest+RCal task; needed by subsequent tasks\n",
    "        \n",
    "        # Now handle the rest of the subtasks (if there are any)\n",
    "        ical_task = None\n",
    "        for i in range(2, nr_subtasks):\n",
    "            subtask = hpso_subtasks[i]\n",
    "            uid += 1\n",
    "            t = SDPTask(uid, str(subtask))\n",
    "            t.set_param(\"data_in\", ingest_rcal_data) # Shared by all these tasks; needs not to be copied every time\n",
    "            # TODO - replace \"buffersize\" as task field. Should be a 'data object' whose movement is modelled separately \n",
    "            t.set_param(\"memsize\", performance_dict[hpso][subtask]['cache'])\n",
    "            t.set_param(\"flopcount\", performance_dict[hpso]['Tobs'] * performance_dict[hpso][subtask]['compRate'])\n",
    "            \n",
    "            if i == 2:\n",
    "                assert subtask in HPSOs.ical_subtasks  # Assumed that this is an ical subtask\n",
    "                t.set_param(\"prec_task\", prev_ingest_task) # Associated ingest task must complete before this one can start\n",
    "                ical_task = t  # Remember this task, as DPrep tasks depend on it\n",
    "            elif subtask in HPSOs.dprep_subtasks:\n",
    "                assert ical_task is not None\n",
    "                t.set_param(\"prec_task\", ical_task, prevent_overwrite=False)\n",
    "\n",
    "            t.data_out  = 0  # TODO: No idea what gets output here. visRate? Temporarily set to zero \n",
    "            \n",
    "            tasks.append(t)\n",
    "    return tasks\n",
    "\n",
    "def sum_deltas(deltas_history, timepoint, sorted_delta_keys=None, value_min=None, value_max=None, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Sums all the deltas chronologically up to the timestamp t\n",
    "    @param deltas_history  : a dictionary that maps wall clock timestamps to a resource's value-changes\n",
    "    @param timepoint       : The time until which the values should be summed\n",
    "    @param sorted_delta_keys : Optional sorted timestamps; prevents re-sorting the timestamps for efficiency\n",
    "    @param value_min       : Lowest allowable value for the resource's balance. Default zero.\n",
    "    @param value_max       : Higest allowable value for the resource's balance. Default None.\n",
    "    @param eps             : Numerical rounding tolerance\n",
    "    @return                : The sum of the deltas from the beginning up to (and including) the timepoint. \n",
    "                             Returns False if value_min of value_max are violated\n",
    "    \"\"\"\n",
    "    timestamps_sorted = sorted_delta_keys\n",
    "    if timestamps_sorted is None:\n",
    "        timestamps_sorted = sorted(deltas_history.keys())\n",
    "    \n",
    "    stop_before_index = bisect.bisect_left(timestamps_sorted, timepoint)\n",
    "    if timepoint in deltas_history:\n",
    "        stop_before_index += 1  # The position found by bisect needs to be included in the summation\n",
    "    assert stop_before_index > 0  # The chosen time point cannot precede the first entry\n",
    "        \n",
    "    value_at_t = 0    \n",
    "    for i in range(stop_before_index):\n",
    "        value_at_t += deltas_history[timestamps_sorted[i]]\n",
    "        if (((value_min is not None) and (value_at_t + eps < value_min)) or \n",
    "            ((value_max is not None) and (value_at_t - eps > value_max))):\n",
    "            warnings.warn(\"Sum of deltas leads to value of %g at time %g sec. Outside imposed bounds of [%s,%s] \" % \n",
    "                          (value_at_t, timestamps_sorted[i], value_min, value_max))\n",
    "            return None\n",
    "        \n",
    "    return value_at_t\n",
    "\n",
    "def find_suitable_time(deltas_history, timepoint, sorted_delta_keys=None, value_min=None, value_max=None, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Finds the smallest time >= t_min, so that the sum of the deltas is between value_min and value_max (if defined)\n",
    "    @param deltas_history  : a dictionary that maps wall clock timestamps to a resource's value-changes\n",
    "    @param timepoint       : The earliest point in time for the insertion\n",
    "    @param sorted_delta_keys : Optional sorted timestamps; prevents re-sorting the timestamps for efficiency\n",
    "    @param value_min       : Lowest allowable value of the resource's balance for insertion.\n",
    "    @param value_max       : Higest allowable value of the resource's balance for insertion.\n",
    "    @param eps             : Numerical rounding tolerance\n",
    "    @return                : The timestamp. None, if none found.\n",
    "    \"\"\"\n",
    "    # First cover the trivial case where there is no requirement for a suitable value at t=timepoint.\n",
    "    if (value_min is None) and (value_max is None):\n",
    "        return timepoint\n",
    "\n",
    "    timestamps_sorted = sorted_delta_keys\n",
    "    if timestamps_sorted is None:\n",
    "        timestamps_sorted = sorted(deltas_history.keys())\n",
    "\n",
    "    value_at_t = sum_deltas(deltas_history, timepoint, timestamps_sorted)\n",
    "\n",
    "    # Check whether the value at the supplied timepoint is suitable. If so we just use this timepoint.\n",
    "    if not (((value_min is not None) and (value_at_t + eps < value_min)) or \n",
    "            ((value_max is not None) and (value_at_t - eps > value_max))):\n",
    "        return timepoint\n",
    "\n",
    "    # Otherwise, we continue searching until we find a suitable timepoint\n",
    "    print(\"... initial task insertion time unsuitable. Searching forward in time...\")    # TODO remove\n",
    "    start_at_index = bisect.bisect_left(timestamps_sorted, timepoint)\n",
    "    if timepoint in deltas_history:\n",
    "        start_at_index += 1  # The position found by bisect was included in the summation; start one position later\n",
    "    assert start_at_index > 0\n",
    "        \n",
    "    for i in range(start_at_index, len(timestamps_sorted)):\n",
    "        value_at_t += deltas_history[timestamps_sorted[i]]\n",
    "        if not (((value_min is not None) and (value_at_t + eps < value_min)) or \n",
    "                ((value_max is not None) and (value_at_t - eps > value_max))):\n",
    "            return timepoint\n",
    "    \n",
    "    # Otherwise, no valid timepoint has been found!\n",
    "    raise Exception(\"No valid time point found!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def add_delta(deltas_history, delta, t_start, t_end=None, value_min=0, value_max=None, sorted_delta_keys=None):\n",
    "    \"\"\"\n",
    "    Applies the delta with proposed start and end times (wall clock) to a resource's simulated value change history.\n",
    "    @param deltas_history  : a dictionary that maps wall clock timestamps to a resource's value-changes\n",
    "    @param delta           : the change that will be added at t_start and reversed at t_end (iff t_end not None)\n",
    "    @param t_start         : the wall clock time when the delta is applied\n",
    "    @param t_end           : the wall clock time at which the delta expires (i.e. is reversed). Default None.\n",
    "    @param value_min       : Lowest allowable value for the resource's balance. Default zero.\n",
    "    @param value_max       : Higest allowable value for the resource's balance. Default None.\n",
    "    @param sorted_delta_keys : Optional sorted timestamps; prevents re-sorting the timestamps for efficiency\n",
    "    \"\"\"    \n",
    "    if t_end is not None:\n",
    "        assert t_end >= t_start\n",
    "    if (value_min is not None) and (value_max is not None):\n",
    "        assert value_max >= value_min\n",
    "        \n",
    "    timestamps_sorted = sorted_delta_keys\n",
    "    if timestamps_sorted is None:\n",
    "        timestamps_sorted = sorted(deltas_history.keys())\n",
    "\n",
    "    # We now insert the deltas into the variable's history, and then sum it until the end of the delta's duration\n",
    "    # to ensure that we do not violate any condition by adding this delta to the history\n",
    "    deltas_new = deltas_history.copy()\n",
    "    \n",
    "    if t_start in deltas_new:\n",
    "        deltas_new[t_start] += delta\n",
    "    else:\n",
    "        deltas_new[t_start] = delta\n",
    "        \n",
    "    if t_end is not None:        \n",
    "        if t_end in deltas_new:\n",
    "            deltas_new[t_end] -= delta\n",
    "        else:\n",
    "            deltas_new[t_end] = -delta\n",
    "\n",
    "    # The step below sums across the whole new delta sequence to make sure that it is valid. Will raise exception if not.\n",
    "    timestamps_sorted = sorted(deltas_new.keys())\n",
    "    value_after = sum_deltas(deltas_new, timestamps_sorted[-1], timestamps_sorted, value_min, value_max, epsilon)\n",
    "\n",
    "    # If return value is not none the addition is valid and we can add the value into the real sequence of deltas\n",
    "    if value_after is not None:\n",
    "        if t_start in deltas_history:\n",
    "            deltas_history[t_start] += delta\n",
    "        else:\n",
    "            deltas_history[t_start] = delta\n",
    "\n",
    "        if t_end is not None:        \n",
    "            if t_end in deltas_history:\n",
    "                deltas_history[t_end] -= delta\n",
    "            else:\n",
    "                deltas_history[t_end] = -delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes  performace requirements for each HPSO using parametric model\n",
    "### We do this once, and store the results in a dictionary for lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance_dict = {}  # A dictionary of dictionaries.  HPSO requirements are computed once and stored as lookups\n",
    "\n",
    "# As a test we loop over all HPSOs we wish to handle, computing results for each\n",
    "for task_letter in sorted(hpso_lookup.keys()):\n",
    "    hpso = hpso_lookup[task_letter]\n",
    "    print('*** Processing task type %s => %s ***\\n' % (task_letter, hpso))\n",
    "    if not hpso in performance_dict:\n",
    "        performance_dict[hpso] = {}\n",
    "        \n",
    "    for subtask in HPSOs.hpso_subtasks[hpso]:\n",
    "        print('subtask -> %s' % subtask)\n",
    "        if not subtask in performance_dict[hpso]:\n",
    "            performance_dict[hpso][subtask] = {}\n",
    "        \n",
    "        cfg = PipelineConfig(hpso=hpso, hpso_subtask=subtask)\n",
    "        (valid, msgs) = cfg.is_valid()\n",
    "        if not valid:\n",
    "            print(\"Invalid configuration!\")\n",
    "            for msg in msgs:\n",
    "                print(msg)\n",
    "            raise AssertionError(\"Invalid config\")\n",
    "        tp = cfg.calc_tel_params()\n",
    "        results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "        \n",
    "        performance_dict[hpso]['Tobs'] = tp.Tobs  # Observation time\n",
    "        performance_dict[hpso][subtask]['ingestRate'] = results[0]\n",
    "        performance_dict[hpso][subtask]['cache'] = results[1]\n",
    "        performance_dict[hpso][subtask]['visRate'] = results[2]\n",
    "        performance_dict[hpso][subtask]['compRate'] = results[3]\n",
    "        \n",
    "        print('Buffer ingest rate\\t= %g TB/s' % results[0])\n",
    "        print('Cache memory\\t= %g TB' % results[1])\n",
    "        print('Visibility IO rate\\t= %g TB/s' % results[2])\n",
    "        print('Compute Rate\\t= %g PetaFLOP/s' % results[3])\n",
    "        print()\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create run a short test sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, simulate the execution of this sequence on the SDP and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = ('A','A','B','B','B','A')\n",
    "seqL = ('A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B')\n",
    "seqM = ('B','G','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','G','C','F','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','G','G','E','E','E','E','D')\n",
    "\n",
    "task_list = task_letters_to_SDPTask_list(seqL, performance_dict)\n",
    "for task in task_list:\n",
    "    print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPSO_letter_sequence = seqL\n",
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "task_list = task_letters_to_SDPTask_list(HPSO_letter_sequence, performance_dict)\n",
    "max_nr_iterations = 1000  # Automatic termination if the loop fails to schedule all taks after this many iterations\n",
    "\n",
    "# First, assert that the SDP has enough FLOP/s capacity to handle the real-time tasks. If not, we can't continue.\n",
    "tasks_to_be_scheduled = set()\n",
    "for task in task_list:\n",
    "    tasks_to_be_scheduled.add(task)\n",
    "    if task.t_fixed is not None:\n",
    "        required_FLOPs = task.flopcount / task.t_fixed\n",
    "        #print(\"Task %d requires a FLOPS rate of %g PetaFLOP/s\" % (task.uid, required_FLOPs))\n",
    "        if (task.flopcount / task.t_fixed) > sdp_FLOPS:\n",
    "            raise AssertionError(\"Task %d (%s) requires %g PetaFLOP/s. SDP capacity of %g PetaFLOP/s is insufficient!\" \n",
    "                                  % (task.uid, task.description, required_FLOPs, sdp_FLOPS) )\n",
    "print('SDP has sufficient FLOPS capacity to handle real-time tasks.')\n",
    "            \n",
    "# Next, iteratively run through all tasks, scheduling them as we go along (where possible). \n",
    "# Scheduling means that each task gets put in a plausible sequence.\n",
    "# Repeat until all tasks are scheduled\n",
    "nr_iterations = 0\n",
    "tasks_to_timestamps = {}  # Maps tasks to completion times\n",
    "timestamps_to_tasks = {}  # Maps completion times to tasks lists\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "wall_clock_advance = False # Iff not true, advance the wall clock to this value\n",
    "\n",
    "flops_deltas  = {0:0}  # maps wall clock times to SDP FLOPS allocation / deallocation (+/-) sizes\n",
    "memory_deltas = {0:0}  # maps wall clock times to SDP working memory (RAM) allocation / deallocation (+/-) sizes\n",
    "hot_buffer_deltas = {0:0}   # maps wall clock times to hot buffer allocation / deallocation (+/-) sizes\n",
    "cold_buffer_deltas = {0:0}  # maps wall clock times to cold buffer allocation / deallocation (+/-) sizes\n",
    "preserve_deltas    = {0:0}  # maps wall clock times to long term preserve allocation sizes (presumably only +)\n",
    "\n",
    "while len(tasks_to_timestamps) < len(tasks_to_be_scheduled):\n",
    "    nr_iterations += 1\n",
    "    print(\"-= Starting iteration %d =-\" % nr_iterations)\n",
    "    nr_tasks_scheduled_this_iteration = 0\n",
    "\n",
    "    if nr_iterations > max_nr_iterations:\n",
    "        print(\"Warning: Maximum number of iterations exceeded; aborting!\")\n",
    "        warnings.warn('Maximum number of iterations exceeded; aborting!')\n",
    "        break\n",
    "    \n",
    "    for task in tasks_to_be_scheduled:\n",
    "        if task in tasks_to_timestamps:\n",
    "            continue  # The task has already been scheduled. Skipping\n",
    "            \n",
    "        elif (task.prec_task is not None) and (task.prec_task not in tasks_to_timestamps):\n",
    "            continue\n",
    "                \n",
    "        elif (task.prec_task is not None) and (task.prec_task in tasks_to_timestamps) and (wall_clock < task.prec_task.t_end):\n",
    "            # Wall clock is less than the end time of the preceding task (on which this task depends).\n",
    "            # Therefore we may need to advance the clock to enable us to schedule any tasks! \n",
    "            if not wall_clock_advance:\n",
    "                wall_clock_advance = task.prec_task.t_end\n",
    "            else:\n",
    "                wall_clock_advance = min(wall_clock_advance, task.prec_task.t_end)\n",
    "        else:\n",
    "            task_flops = 0\n",
    "            if (task.t_fixed is not None):\n",
    "                task_flops = task.flopcount / task.t_fixed\n",
    "            else:\n",
    "                flops_in_use = sum_deltas(flops_deltas, wall_clock, value_max=sdp_FLOPS)\n",
    "                task_flops = max(1, (sdp_FLOPS - flops_in_use) * 0.5)  # TODO: this may be changed\n",
    "\n",
    "            start_time = find_suitable_time(flops_deltas, wall_clock, value_max=(sdp_FLOPS-task_flops), eps=epsilon)\n",
    "            t_start = start_time\n",
    "            t_end = start_time + (task.flopcount / task_flops)\n",
    "\n",
    "            if hasattr(task, \"t_start\"):\n",
    "                raise Exception(\"Task already has t_start defined!\\n %s\" % str(task))\n",
    "            task.set_param(\"t_start\", t_start)\n",
    "            task.set_param(\"t_end\", t_end)\n",
    "\n",
    "            add_delta(flops_deltas, task_flops, t_start, t_end, value_min=0, value_max=sdp_FLOPS)             \n",
    "            add_delta(memory_deltas, task.memsize, t_start, t_end, value_min=0)             \n",
    "            #add_delta(flops_deltas, flops_required, t_start, t_end, value_min=0, value_max=sdp_FLOPS)             \n",
    "            #add_delta(flops_deltas, flops_required, t_start, t_end, value_min=0, value_max=sdp_FLOPS)             \n",
    "            #add_delta(flops_deltas, flops_required, t_start, t_end, value_min=0, value_max=sdp_FLOPS)             \n",
    "\n",
    "            # Add this task to the 'tasks_to_timestamps' and 'timestamps_to_tasks' mappings\n",
    "            tasks_to_timestamps[task] = t_end\n",
    "            if t_end in timestamps_to_tasks:\n",
    "                timestamps_to_tasks[t_end].append(task)\n",
    "            else:\n",
    "                timestamps_to_tasks[t_end] = [task]\n",
    "                \n",
    "            print('* Scheduled Task %d at wall clock time %g sec. Ends at t=%g sec. ' % (task.uid, t_start, t_end))\n",
    "            nr_tasks_scheduled_this_iteration += 1\n",
    "        \n",
    "    print('Number of scheduled tasks after %d iterations is %d out of %d' % (nr_iterations, len(tasks_to_timestamps), \n",
    "                                                                            len(tasks_to_be_scheduled)))\n",
    "\n",
    "    if (nr_tasks_scheduled_this_iteration == 0):\n",
    "        if wall_clock_advance:\n",
    "            print(\"-> Advancing wall clock to %g sec.\" % wall_clock_advance)\n",
    "            wall_clock = wall_clock_advance\n",
    "            wall_clock_advance = False\n",
    "        else:\n",
    "            print(\"Warning! No tasks scheduled, and wall clock not advanced!\")\n",
    "            \n",
    "'''\n",
    "    # Here are the task fields, for referece: \n",
    "    uid         = None  # Unique ID - must be defined at task creation. Used for hashing and equality.\n",
    "    description = None  # A human-readable description of the task\n",
    "    t_min_start = None  # Earliest wall clock time (in seconds) that this task can / may start\n",
    "    prec_task   = None  # Preceding task that needs to complete before this one can start (can be None)\n",
    "    t_fixed     = None  # fixed minimum duration (in seconds) of this task (e.g. for an observation)\n",
    "    data_in     = None  # Amount of data (in TB) this task requires to read from (presumably hot) buffer *before* starting\n",
    "    memsize     = None  # The amount of SDP working memory (in TB) needed to perform this task\n",
    "    flopcount   = None  # Number of operations (in PetaFLOP) required to complete this task\n",
    "    data_out    = None  # Amount of data (in TB) this task stores *after* finishing (written to buffer or long-term preserve)\n",
    "    data_source = None  # Data Location\n",
    "    data_target = None  # Data Location\n",
    "'''     \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deltas(deltas, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
    "    timestamps_sorted = sorted(deltas.keys())\n",
    "    x_axis = []\n",
    "    y_axis = []\n",
    "    \n",
    "    value = 0\n",
    "    for t in timestamps_sorted:\n",
    "        time_hours = t / 3600\n",
    "        x_axis.append(time_hours)\n",
    "        y_axis.append(value)\n",
    "        value += deltas[t]\n",
    "        x_axis.append(time_hours)\n",
    "        y_axis.append(value)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlim(min(x_axis), max(x_axis)*1.05)\n",
    "    plt.ylim(min(y_axis), max(y_axis)*1.05)\n",
    "\n",
    "plot_deltas(flops_deltas, 'Evolution of SDP FLOP/s', 'wall clock time (hours)', 'PetaFLOP/s')\n",
    "plot_deltas(memory_deltas, 'Evolution of SDP working memory (RAM)', 'wall clock time (hours)', 'TeraByte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook incomplete beyond this point - don't execute unless you know what you're doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = {}\n",
    "t[1] = 'a'\n",
    "t[3] = 'b'\n",
    "t.pop(3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "for task in tasks:\n",
    "    if hasattr(task, )\n",
    "\n",
    "# Run through the list of tasks, determining start and end times for their execution and the effect on the buffer\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary mapping wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "uids_completed = set()\n",
    "\n",
    "for task in tasks:\n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n",
    "\n",
    "buffer_evolution = collections.OrderedDict(sorted(buffer_deltas.items()))\n",
    "time_vals   = np.zeros(2 * len(buffer_evolution))\n",
    "buffer_vals = np.zeros(2 * len(buffer_evolution))\n",
    "\n",
    "i = 0\n",
    "buffer_val = 0\n",
    "time_val   = 0\n",
    "for k, delta in buffer_evolution.items(): \n",
    "    #print('(%.1f,\\t%.2f)' % (k/3600, delta))\n",
    "    time_val = k / 3600  # hours\n",
    "    time_vals[i]     = time_val\n",
    "    buffer_vals[i]   = buffer_val\n",
    "    buffer_val += delta  # Adds the buffer delta to the buffer's stored contents\n",
    "    time_vals[i+1]   = time_val  # we assume no time went by (writing being instantaneous)\n",
    "    buffer_vals[i+1] = buffer_val  # TeraBytes\n",
    "    i += 2\n",
    "\n",
    "plt.plot(time_vals, buffer_vals / 1e3, 'b-')\n",
    "plt.title('Evolution of the SDP Buffer while executing the supplied LOW sequence.\\nObservation time = %.1f hrs.' \n",
    "          ' Total execution time = %.1f hrs; Max buffer = %.1f PB' % (wall_clock / 3600, time_vals[-1],                                                                      np.max(buffer_vals)/1e3))\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('buffer usage (PB)')\n",
    "plt.xlim(0, time_vals[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(range(len(idle_time_durations))), idle_time_durations / 3600, marker='s', color = 'r', linewidth=0)\n",
    "plt.title('Idle time that tasks spend in the LOW buffer.\\nSummed idle time for all tasks = %.1f hrs.' % \n",
    "          (np.sum(idle_time_durations) / 3600))\n",
    "plt.xlabel('Task''s number in sequence')\n",
    "plt.ylabel('Time (hours)')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-coded performace costs and requirements from Rosie's Excel sheet\n",
    "### These were previously used in rev [3372fdd] to approximately replicate Rosie's results. Check (rerun) the notebook at that repository revision to regenerate those results - not repeated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following sets of values should be computed using the parametric model. Just hard-coded for now (from Excel)\n",
    "hpso_ingest_rates = {'A':0.459, 'B':3e-3, 'C':0.117, 'D':0.112, 'E':0.0603, 'F':0.244, 'G':0.438}  # in TeraByte/s\n",
    "# FLOPcounts below are the PetaFLOPs required to process one second of ingested data\n",
    "hpso_flopcounts = {'A':50.4, 'B':2.0, 'C':7.5, 'D':6.2, 'E':2.9833, 'F':17.689, 'G':27.698}  # in PetaFLOP/s\n",
    "hpso_durations  = {'A':6, 'B':0.17, 'C':6, 'D':6, 'E':4.4, 'F':0.1233, 'G':6}  # in hours -- TODO check whether correct\n",
    "\n",
    "sdp_setup_time = 60  # the minimum amount of time between processing tasks on the SDP (seconds)\n",
    "telecope_setup_time = 0  # TODO is this correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of \"Low\" and \"Mid\" sequences from Rosie's Excel sheet\n",
    "### Create a lists of observation tasks as letter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqL = ('A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','A','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B')\n",
    "seqM = ('B','G','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','G','C','F','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','B','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','F','G','G','E','E','E','E','D')\n",
    "\n",
    "print('HPSO LOW task distribution (number of occurences) A..B = (%.0f, %.0f)' % (seqL.count('A'), seqL.count('B')))\n",
    "tA = seqL.count('A') * hpso_durations['A']\n",
    "tB = seqL.count('B') * hpso_durations['B']\n",
    "print('HPSO LOW task distribution (observation time) A..B = (%.1f%%, %.1f%%)' % (100 * tA / (tA + tB), 100 * tB / (tA + tB)))\n",
    "\n",
    "tA = seqM.count('A')\n",
    "tB = seqM.count('B')\n",
    "tC = seqM.count('C')\n",
    "tD = seqM.count('D')\n",
    "tE = seqM.count('E')\n",
    "tF = seqM.count('F')\n",
    "tG = seqM.count('G')\n",
    "tt = len(seqM)\n",
    "\n",
    "print('\\nHPSO MID task distribution (number of occurences) A..G = (%.0f, %.0f, %.0f, %.0f, %.0f, %.0f, %.0f)' % \\\n",
    "      (tA, tB, tC, tD, tE, tF, tG))\n",
    "print('HPSO MID task distribution (observation time) A..G = (%.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%, %.1f%%)' % \\\n",
    "      (100*tA/tt, 100*tB/tt, 100*tC/tt, 100*tD/tt, 100*tE/tt, 100*tF/tt, 100*tG/tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the lists of letters to build a lists of task objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtually execute the \"seqL\" task list for LOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = task_letters_to_objects(seqL, performance_dict)  # Set up the task list from the letter sequence\n",
    "\n",
    "sdp_FLOPS = 22.8  # NB: The processing capacity of the SDP in PetaFLOP/s\n",
    "\n",
    "# Run through the list of tasks, determining start and end times for their execution and the effect on the buffer\n",
    "\n",
    "wall_clock = 0       # Simulated wall clock time (seconds)\n",
    "buffer_deltas = {}   # a dictionary mapping wall clock times to buffer allocation / deallocation (+/-) sizes\n",
    "t_proc_end_last = 0  # The wall clock time that the last process completed\n",
    "idle_time_durations = np.zeros(len(tasks))  # in seconds\n",
    "i = 0\n",
    "uids_completed = set()\n",
    "\n",
    "for task in tasks:\n",
    "    task.t_obs_start = wall_clock\n",
    "    add_delta(buffer_deltas, task.t_obs_start, task.bufsize)\n",
    "    t_obs_end = task.t_obs_start + task.t_obs  # Time the observation completes\n",
    "    task.t_proc_start = max(t_obs_end, t_proc_end_last + sdp_setup_time)\n",
    "    task.t_proc_end   = task.t_proc_start + task.flopcount * task.t_obs / sdp_FLOPS\n",
    "    add_delta(buffer_deltas, task.t_proc_end, -task.bufsize)\n",
    "    t_proc_end_last = task.t_proc_end\n",
    "    wall_clock = t_obs_end + telecope_setup_time\n",
    "    idle_time_durations[i] = task.t_proc_start - t_obs_end\n",
    "    i += 1\n",
    "\n",
    "buffer_evolution = collections.OrderedDict(sorted(buffer_deltas.items()))\n",
    "time_vals   = np.zeros(2 * len(buffer_evolution))\n",
    "buffer_vals = np.zeros(2 * len(buffer_evolution))\n",
    "\n",
    "i = 0\n",
    "buffer_val = 0\n",
    "time_val   = 0\n",
    "for k, delta in buffer_evolution.items(): \n",
    "    #print('(%.1f,\\t%.2f)' % (k/3600, delta))\n",
    "    time_val = k / 3600  # hours\n",
    "    time_vals[i]     = time_val\n",
    "    buffer_vals[i]   = buffer_val\n",
    "    buffer_val += delta  # Adds the buffer delta to the buffer's stored contents\n",
    "    time_vals[i+1]   = time_val  # we assume no time went by (writing being instantaneous)\n",
    "    buffer_vals[i+1] = buffer_val  # TeraBytes\n",
    "    i += 2\n",
    "\n",
    "plt.plot(time_vals, buffer_vals / 1e3, 'b-')\n",
    "plt.title('Evolution of the SDP Buffer while executing the supplied LOW sequence.\\nObservation time = %.1f hrs.' \n",
    "          ' Total execution time = %.1f hrs; Max buffer = %.1f PB' % (wall_clock / 3600, time_vals[-1],                                                                      np.max(buffer_vals)/1e3))\n",
    "plt.xlabel('time (hours)')\n",
    "plt.ylabel('buffer usage (PB)')\n",
    "plt.xlim(0, time_vals[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.array(range(len(idle_time_durations))), idle_time_durations / 3600, marker='s', color = 'r', linewidth=0)\n",
    "plt.title('Idle time that tasks spend in the LOW buffer.\\nSummed idle time for all tasks = %.1f hrs.' % \n",
    "          (np.sum(idle_time_durations) / 3600))\n",
    "plt.xlabel('Task''s number in sequence')\n",
    "plt.ylabel('Time (hours)')\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#               Table Row Title            Unit     Default?  Sum?   Expression\n",
    "results_map =[('Total buffer ingest rate','TeraBytes/s',True, False, lambda tp: tp.Rvis_ingest*tp.Nbeam*tp.Npp*tp.Mvis/c.tera),\n",
    "              ('Working (cache) memory',  'TeraBytes',  True, True,  lambda tp: tp.Mw_cache/c.tera,   ),\n",
    "              ('Visibility I/O Rate',     'TeraBytes/s',True, True,  lambda tp: tp.Rio/c.tera,        ),\n",
    "              ('Total Compute Req',       'PetaFLOP/s', True, True,  lambda tp: tp.Rflop/c.peta,      ),\n",
    "              ('Comp Req Breakdown ->',   'PetaFLOP/s', True, True,  lambda tp: tp.get_products('Rflop', scale=c.peta), )]\n",
    "del results_map[4]  # We actually don't care about the breakdown for now; but it is useful to know how to get it\n",
    "\n",
    "hpso = hpso_lookup['A']  # hpso01.ICAL\n",
    "\n",
    "cfg = PipelineConfig(hpso=hpso)\n",
    "assert cfg.is_valid()\n",
    "tp = cfg.calc_tel_params()\n",
    "\n",
    "results = iapi._compute_results(cfg, False, results_map)  #TODO - refactor this method's parameter sequence\n",
    "print('Cache memory for hpso01.ICAL = %g TB' % results[1])\n",
    "print('Visibility rate for hpso01.ICAL = %g TB/s' % results[2])\n",
    "print('Rflop for hpso01.ICAL = %g PetaFLOPS' % results[3])\n",
    "\n",
    "# Another, slightly more roundabout, way to do the same as _compute_results \n",
    "# (tsnap_opt, nfacet_opt) = imp.find_optimal_Tsnap_Nfacet(tp)\n",
    "# result_expressions = iapi.get_result_expressions(results_map, tp)\n",
    "# results_for_pipeline = imp.evaluate_expressions(result_expressions, tp, tsnap_opt, nfacet_opt)\n",
    "# print(results_for_pipeline[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code taken from computing parametric model results by Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teles = (Telescopes.SKA1_Low, Telescopes.SKA1_Mid)\n",
    "bands = (Bands.Low, \n",
    "         Bands.Mid1, Bands.Mid2, Bands.Mid5A, Bands.Mid5B, Bands.Mid5C,\n",
    "         Bands.Sur1)\n",
    "parallel = 0  # Set this to 0 if PyMP is absent\n",
    "\n",
    "for pipeline in Pipelines.all:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % pipeline, teles, bands, [pipeline],\n",
    "                              parallel=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for band in bands:\n",
    "    iapi.stack_bars_pipelines(\"%s Computational Requirements [PetaFLOP/s]\" % band, teles, [band], Pipelines.all,\n",
    "                              parallel = parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iapi.stack_bars_hpsos(\"HPSOs Computational Requirements [PetaFLOP/s]\", HPSOs.hpsos,\n",
    "                      parallel=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

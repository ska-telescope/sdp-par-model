{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SDP Working Set & Communication Analysis\n",
    "\n",
    "This notebook attempts to calculate how much working memory and communication we need to run SDP pipelines. See SDP Memo 038 (Pipeline Working Sets & Communication) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import math\n",
    "from ipywidgets import ToggleButtons, Text, Layout, interactive\n",
    "sys.path+=['..']\n",
    "from sdp_par_model import reports\n",
    "from sdp_par_model.config import PipelineConfig\n",
    "from sdp_par_model.parameters.definitions import *\n",
    "from sdp_par_model.parameters.definitions import Constants as c\n",
    "\n",
    "verbose_display     = ['Overview', 'Details', 'Debug']\n",
    "def toggles(opts, *args, **kwargs): return ToggleButtons(options=opts, *args, **kwargs)\n",
    "def adjusts(*args): return Text(*args, layout=Layout(width='100%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {return false;}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibilities\n",
    "\n",
    "The assumption is that visibilities expose us to the greatest memory pressure, as they are large and multiply up quickly once you consider that we also need model and facet visibilities around. Therefore we should stream them in small chunks. Here we calculate the visibility rates, and how much time we have to work on them before we overflow buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry(title, val, unit='', doc=''):\n",
    "    return (title, val, unit, doc)\n",
    "def visibility_analysis(tp, Qspeed, Nnode, Npredict, Mvisnode):\n",
    "    \n",
    "    Mbuf = tp.Npp * tp.Rvis.eval_sum(tp.baseline_bins) * tp.Nbeam * tp.Mvis * tp.Tobs\n",
    "    Rvis = tp.Mvis * tp.Nbeam * tp.Npp * tp.Rvis_ingest\n",
    "    Nmajor = 1 + tp.Nmajortotal\n",
    "    Rvisnode = Qspeed * (Rvis * Nmajor) / float(Nnode)\n",
    "    Rpredictnode = Npredict * Rvisnode\n",
    "    twork = Mvisnode * c.giga / (Rvisnode + Rpredictnode)\n",
    "    Msnapnode = tp.Mvis * tp.Npp * tp.Rvis_ingest * tp.Tsnap / Nnode\n",
    "    Cmemprice = 0.5 # €/GB\n",
    " \n",
    "    return [\n",
    "        entry('-- Visibilities --', None, None),\n",
    "        entry('Q_speed = ', Qspeed, '', 'Compute time scale compared with ingest'),\n",
    "        entry('t_major = ', tp.Tobs / Qspeed / Nmajor, 's', 'Average time we have to run one major loop'),\n",
    "        entry('M_buf/node =', Mbuf / Nnode / c.giga, 'GB', 'Visibility buffer size we need per node to hold the observation'),\n",
    "        entry('R_vis/node = ', Rvisnode / c.giga, 'GB/s', 'Minimum visibility buffer read rate to sustain to read visibilities once per major loop'),\n",
    "        entry('M_snap/node = ', Msnapnode / c.giga, \"GB\", 'Size of snapshot data per node (without predict)'),\n",
    "        entry('R_predict/node = ', Rpredictnode / c.giga, 'GB/s', 'Rate we need to produce predicted visibilities at to match visibility buffer rate'),\n",
    "        entry('M_vis/node =', Mvisnode, 'GB', 'Assumed memory we have per node for holding visibilities'),\n",
    "        entry('t_work =', twork, 's', 'Time we can work on every visibility before we run out of memory'),\n",
    "        entry('C_work =', Nnode * Cmemprice * (Rvisnode + Rpredictnode) / c.giga, '€/s', 'Cost for extending the above number by one second')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging\n",
    "\n",
    "The second most important data items is images (or grids). Especially for the Mid telescope, those are going to be large enough that we cannot afford to have too many copies of them lying around.\n",
    "\n",
    "Therefore the idea here is that we distribute image data around the cluster, and have visibility streaming processes load/write the data \"on demand\". This can cause significant amount of communication unless we permit at least some redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size(tp):\n",
    "    Mfacet = tp.Mpx * tp.Npix_linear**2\n",
    "    Mimage = Mfacet * tp.Nfacet**2\n",
    "    return Mfacet, Mimage\n",
    "\n",
    "def imaging_analysis(tp, Qspeed, Nnode, Nsubbands):\n",
    "    Mfacet, Mimage = get_image_size(tp)\n",
    "    \n",
    "    # Calculating image size including facetting, and therefore overlap. I think this is more fair.\n",
    "    Mimagenodemin = max(Nsubbands * tp.Npp * tp.Ntt * Mimage / Nnode, Mfacet)\n",
    "    Rphrot = Qspeed * (tp.products.get(Products.PhaseRotationPredict,{}).get('Rflop',0) + \\\n",
    "                       tp.products.get(Products.PhaseRotation,{}).get('Rflop',0))\n",
    "    \n",
    "    return [\n",
    "        entry('-- Imaging --', None, None),\n",
    "        entry('N_subbands = ', Nsubbands, '', 'Minimum number of frequencies we *need* to image separately'),\n",
    "        entry('N_pp N_tt M_image = ', tp.Npp * tp.Ntt * Mimage / c.tera, 'TB', 'Continuum image/grid size - data a single visibility updates'),\n",
    "        entry('N_facet = ', tp.Nfacet, '', 'How much we split the image into facets'),\n",
    "        entry('R_phaserotate = ', Rphrot / c.peta, 'Pflop/s', 'Compute cost for facetting (imaging+predict)'),\n",
    "        entry('M_image/node,max = ', tp.Npp * tp.Ntt * Mfacet / c.giga, 'GB', 'Facet data per node, assuming no distribution in polarisation/Taylor terms'),\n",
    "        entry('M_image/node,min = ', Mimagenodemin / c.giga, 'GB', 'Facet data per node, assuming complete distribution in polarisation/Taylor terms')\n",
    "    ]\n",
    "\n",
    "def _get_distribution(tp, Nnode, Nsubbands, Ndist_pptt):\n",
    "    Ndist_ft = max(1, float(Nnode / (Nsubbands * tp.Nfacet**2 * Ndist_pptt)))\n",
    "    Nseq = Nsubbands * tp.Nfacet**2 * Ndist_pptt * Ndist_ft / Nnode\n",
    "    return Ndist_ft, Nseq\n",
    "\n",
    "def imaging_data_analysis(tp, Qspeed, Nnode, Nisland, Nsubbands, Ndist_pptt):\n",
    "    Mfacet, Mimage = get_image_size(tp)\n",
    "    Ndist_ft, Nseq = _get_distribution(tp, Nnode, Nsubbands, Ndist_pptt)\n",
    "    \n",
    "    Mimagenode = max(Ndist_ft * Nsubbands * tp.Npp * tp.Ntt * Mimage / Nnode / Nseq, Mfacet)\n",
    "    \n",
    "    Nmajor = 1 + tp.Nmajortotal\n",
    "    Tsnapwork = tp.Tsnap / Qspeed / Nmajor / Nseq\n",
    "    Rimagenode = Mimagenode / Tsnapwork\n",
    "    Tobswork = tp.Tobs / Qspeed / Nmajor / Nseq\n",
    "    Routnode = Mimagenode / Tobswork\n",
    "    \n",
    "    entries_data = [\n",
    "        entry('-- Imaging Data --', None, None),\n",
    "        entry('N_dist,pp/tt = ', Ndist_pptt, '', 'Distribution degree in polarisation / Taylor Terms'),\n",
    "        entry('N_dist,f/t = ', Ndist_ft, '', 'Remaining distribution degree in frequency / time to get enough parallelism'),\n",
    "        entry('N_seq = ', Nseq, '', ''),\n",
    "        entry('M_image/node = ', Mimagenode / c.giga, 'GB', 'Resulting facet data per node'),\n",
    "        entry('t_snap,work = ', Tsnapwork, 's', 'Time every node has to work on a snapshot (assuming no time distribution)'),\n",
    "        entry('R_image/node = ', Rimagenode / c.giga, 'GB/s', 'Data rate for \"spilling\" facets to storage every snapshot'),\n",
    "        entry('R_out/node = ', Routnode / c.giga, 'GB/s', 'Data rate for \"spilling\" facets to storage every snapshot'),\n",
    "    ]\n",
    "\n",
    "    # Number of nodes visibility data needs to be distributed to. Frequency/time distribution\n",
    "    # doess not need communication, as we can assume visibilities to have been appropriately\n",
    "    # distributed beforehand\n",
    "    Ncopy = max(1, Nseq * Nnode / Ndist_ft / Nsubbands) # Nfacet**2 * Ndist_pptt\n",
    "    Rfacet = Qspeed * float(tp.Rfacet_vis) / Nnode\n",
    "\n",
    "    # Every node needs to send a facet visibility stream to Ncopy-1 nodes\n",
    "    Rdistributenode = Qspeed * tp.Rfacet_vis / Nnode * (Ncopy - 1)\n",
    "\n",
    "    # Every island node needs to send at least (Ncopy-Nisland) off-island nodes a facet\n",
    "    # visibility stream. Stream emitted by an island is therefore Nisland times higher.\n",
    "    Rdistributeisland = Qspeed * tp.Rfacet_vis / Nnode * max(0, Ncopy - Nisland) * Nisland\n",
    "    \n",
    "    return Mimagenode, Rdistributenode, Rdistributeisland, entries_data + [\n",
    "        entry('-- Imaging Communication --', None, None),\n",
    "        entry('N_copy = ', Ncopy, '', 'Number of times we need to copy (facet!) visibilities from each node'),\n",
    "        entry(\"R_facet =\", Rfacet / c.mega, \"MB/s\"),\n",
    "        entry('R_distribute/node =', float(Rdistributenode / c.giga), \"GB/s\", 'Facet visibility output and input rate of every node'),\n",
    "        entry('R_distribute/island =', float(Rdistributeisland / c.giga), \"GB/s\", 'Facet visibility output and input rate of every island'),\n",
    "        entry('R_distribute/island2island =', float(Qspeed * tp.Rfacet_vis / Nnode * Nisland**2 / c.giga), \"GB/s\", 'Maximum visibility exchange between pair of islands involved in all-to-all')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Finally we need to calibrate the instrument, which involves solving a number of calibration problems with (very) different scopes into the visibility data. To prevent this from blowing up our working set, we assume that calibration problem input can be reduced in a map/reduce manner. To be specific, we assume that by the time we actually start solving a particular particular problem, we have reduced the input data down to something smaller than 2 visibilities per slot.\n",
    "\n",
    "This allows us to calculate the overhead of calibration with a simple tree reduction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_analysis(tp, Qspeed, Nnode, Nsubbands, Ndist_pptt):\n",
    "    \n",
    "    # 2 sets (model+predicted) of visibilities per polarisation and baseline\n",
    "    Ndist_ft, Nseq = _get_distribution(tp, Nnode, Nsubbands, Ndist_pptt)\n",
    "    Tsnapwork = tp.Tsnap / Qspeed / (1 + tp.Nmajortotal) / Nseq\n",
    "    \n",
    "    Mcal = 2 * tp.Mvis * tp.Npp * tp.Nbl\n",
    "    MGcal = Mcal * tp.Ncal_G_obs / Nnode\n",
    "    MBcal = Mcal * tp.Ncal_B_obs / Nnode\n",
    "    MIcal = Mcal * tp.Ncal_I_obs / Nnode\n",
    "    RGcal_snap = Mcal * tp.Ncal_G_solve / Tsnapwork\n",
    "    RBcal_snap = Mcal * tp.Ncal_B_solve / Tsnapwork\n",
    "    RIcal_snap = Mcal * tp.Ncal_I_solve / Tsnapwork    \n",
    "    \n",
    "    return MGcal+MBcal+MIcal, RGcal_snap + RBcal_snap + RIcal_snap, [\n",
    "        entry('-- Calibration --', None, None),\n",
    "        entry('M_cal = ', Mcal / c.mega, \"MB\", 'Size of input into an individual calibration \"problem\"'),\n",
    "        entry('', ['Gain', 'Band', 'Ion'], \"\", ''),\n",
    "        entry('N_cal = ', [tp.Ncal_G_obs, tp.Ncal_B_obs, tp.Ncal_I_obs], \"k\", 'Calibration problem count'),\n",
    "        entry('M_cal/node = ', [MGcal / c.giga, MBcal / c.giga, MIcal / c.giga], \"GB\", 'Calibration working set per node (assuming perfect distribution)'),\n",
    "        entry('N_cal,snap = ', [tp.Ncal_G_solve, tp.Ncal_B_solve, tp.Ncal_I_solve], \"\",\n",
    "              'Number of calibration problems overlapping a snapshot and subband'),\n",
    "        entry('M_cal,snap = ', [Mcal*tp.Ncal_G_solve/c.giga, Mcal*tp.Ncal_B_solve/c.giga, Mcal*tp.Ncal_I_solve/c.giga], \"GB\",\n",
    "              'Number of calibration problems overlapping a snapshot and subband'),\n",
    "        entry('R_cal,snap = ', [RGcal_snap / c.giga, RBcal_snap / c.giga, RIcal_snap / c.giga], \"GB/s\",\n",
    "              'Data rate per node for exchanging calibration problems after every snapshot')\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In the end, the working set is given by:\n",
    "\n",
    "1. Streaming visibilities, given as parameter\n",
    "2. Two images+grids (predict+imaging)\n",
    "3. The calibration problems\n",
    "\n",
    "And communication rates are given by the sum of:\n",
    "\n",
    "1. Facet streams for imaging (predict + imaging)\n",
    "2. Calibration streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_working_set(telescope, band, pipeline, adjusts,\n",
    "                          Nnode=1500, Nisland=56,\n",
    "                          Rflop=15, Npredict=4, Mvisnode=64,\n",
    "                          Ntt=3, Nfacet=7, Ndist_pptt=4):\n",
    "    \"\"\"\n",
    "    :param Nnode: Number of nodes in cluster\n",
    "    :param Nisland: Number of nodes in a compute island\n",
    "    :param Qspeed: How much faster we need to process compared with ingest\n",
    "       (to leave space for other pipelines to run)\n",
    "    :param Npredict: Number of separate predicts we need to do\n",
    "    :param Mvisnode: Memory per node to cache raw visibilities [GB]\n",
    "    :param Ntt: Number of Taylor terms\n",
    "    :param Nfacet: Number of facets (horizontal+vertical)\n",
    "      Reduces working set and increases distribution, but requires more flops (phase rotation)\n",
    "    :param Ndist_pptt: Degree of distribution in polarisation and Taylor terms.\n",
    "      Reduces working set and increases distribution, but also increases communication.\n",
    "    \"\"\"\n",
    "\n",
    "    adjusts_ = (\"Nfacet=%d Ntt=%d\" % (Nfacet, Ntt)) + adjusts\n",
    "    config = PipelineConfig(telescope, pipeline, band, adjusts=adjusts_)\n",
    "    if not config.is_valid()[0]:\n",
    "        print(*config.is_valid()[1])\n",
    "        return\n",
    "    \n",
    "    # Do calculations\n",
    "    tp = config.calc_tel_params()\n",
    "    Nsubbands = tp.Nsubbands\n",
    "    if pipeline in [Pipelines.DPrepB, Pipelines.DPrepC]:\n",
    "        Nsubbands = tp.Nf_out\n",
    "    Qspeed = Rflop / float(tp.Rflop) * c.peta\n",
    "        \n",
    "    # Perform analyses\n",
    "    entries = []\n",
    "    def add_entry(title, val, unit='', doc=''):\n",
    "        entries.append((title, val, unit, doc))\n",
    "    entries.extend(visibility_analysis(tp, Qspeed, Nnode, Npredict, Mvisnode))\n",
    "    entries.extend(imaging_analysis(tp, Qspeed, Nnode, Nsubbands))\n",
    "    Mws_img, Rnode_img, Risland_img, img_entries = \\\n",
    "      imaging_data_analysis(tp, Qspeed, Nnode, Nisland, Nsubbands, Ndist_pptt)\n",
    "    entries.extend(img_entries)\n",
    "    Mws_cal, Rcal, cal_entries = calibration_analysis(tp, Qspeed, Nnode, Nsubbands, Ndist_pptt)\n",
    "    entries.extend(cal_entries)\n",
    "    \n",
    "    # Compose summary\n",
    "    add_entry('-- Summary --', None, None)\n",
    "    Mworkset = Mvisnode * c.giga + 4 * Mws_img + Mws_cal\n",
    "    add_entry('Mworkset/node = ', Mworkset / c.giga, \"GB\")\n",
    "    add_entry('Rnode = ', (2 * Rnode_img + Rcal) / c.giga, \"GB/s\")\n",
    "    add_entry('Risland = ', (2 * Risland_img + Rcal) / c.giga, \"GB/s\")\n",
    "    add_entry('Rflop = ', Qspeed * float(tp.Rflop) / c.peta, 'Pflop/s')\n",
    "    add_entry('Rflop/node = ', Qspeed * float(tp.Rflop) / Nnode / c.tera, 'Tflop/s')\n",
    "    reports.show_table(\"Working Set Analysis\", *zip(*entries))\n",
    "    \n",
    "# Make interactive layout, reserving a fairly large area to prevent \"blinking\"\n",
    "widget = interactive(calculate_working_set,\n",
    "                     telescope=toggles(Telescopes.available_teles),\n",
    "                     band=toggles(Bands.available_bands),\n",
    "                     pipeline=toggles(Pipelines.imaging, value=Pipelines.ICAL),\n",
    "                     adjusts=adjusts(\"Tobs=6*3600\"),\n",
    "                     Nnode=(1,3000), Nisland=(1,500),\n",
    "                     Rflop=(1,20,0.1), Npredict=(1,10), Mvisnode=(1,128),\n",
    "                     Ntt=(1,5), Nfacet=(1,16), Ndist_pptt=(1,4*4))\n",
    "# widget.layout.height = '500px'\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "d1b63bfef51541c1a2aa71f7d036e334": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP Schedule Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import numpy\n",
    "from ipywidgets import interact_manual, SelectMultiple\n",
    "from IPython.display import display, Markdown\n",
    "from matplotlib import pylab\n",
    "import matplotlib.lines\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from sdp_par_model.scheduling import efficiency\n",
    "\n",
    "from sdp_par_model import reports, config\n",
    "from sdp_par_model.scheduling import graph, level_trace, scheduler\n",
    "from sdp_par_model.scheduling.simulation import ScheduleSimulation\n",
    "from sdp_par_model.parameters import definitions\n",
    "from sdp_par_model.parameters.definitions import Telescopes, Pipelines, Constants, HPSOs\n",
    "from sdp_par_model import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = ScheduleSimulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall observatory selection & capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"low-adjusted\"\n",
    "\n",
    "app.observatory_sizes_and_rates(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read HPSO performance characteristics\n",
    "\n",
    "Loads high performance science objective characteristics generated by the export notebook. This picks up the latest file checked into Git by default, but a csv file path can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.read_hpso_csv(csv_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine computational capacity required for realtime processing\n",
    "\n",
    "As SKA SDP needs to be able to change observation at arbitrary times, we need to always reserve enough computational resources to deal with the most expensive case. Here we figure this out automatically based on the calculated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_flops = 0\n",
    "realtime_flops_hpso = None\n",
    "for hpso in definitions.HPSOs.all_hpsos:\n",
    "    if definitions.HPSOs.hpso_telescopes[hpso] != telescope:\n",
    "        continue\n",
    "    # Sum FLOP rates over involved real-time pipelines\n",
    "    rt_flops = 0\n",
    "    for pipeline in definitions.HPSOs.hpso_pipelines[hpso]:\n",
    "        cfg_name = config.PipelineConfig(hpso=hpso, pipeline=pipeline).describe()\n",
    "        flops = int(math.ceil(float(reports.lookup_csv(csv, cfg_name, 'Total Compute Requirement')) * definitions.Constants.peta))\n",
    "        if pipeline in definitions.Pipelines.realtime:\n",
    "            rt_flops += flops\n",
    "    # Dominates?\n",
    "    if rt_flops > realtime_flops:\n",
    "        realtime_flops = rt_flops\n",
    "        realtime_flops_hpso = hpso\n",
    "        \n",
    "# Show\n",
    "print(\"Realtime processing requirements:\")\n",
    "batch_flops = total_flops - realtime_flops\n",
    "print(\" {:.3f} Pflop/s real-time (from {}), {:.3f} Pflop/s left for batch\".format(\n",
    "    realtime_flops / definitions.Constants.peta,\n",
    "    realtime_flops_hpso, batch_flops / definitions.Constants.peta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive capacities\n",
    "\n",
    "Now that we know the split between batch and realtime processing, we can formulate the capacity dictionary that will be used in scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities = {\n",
    "    graph.Resources.Observatory: 1,\n",
    "    graph.Resources.BatchCompute: batch_flops,\n",
    "    graph.Resources.RealtimeCompute: realtime_flops,\n",
    "    graph.Resources.InputBuffer: input_buffer_size * cold_rate_per_size,\n",
    "    graph.Resources.HotBuffer: hot_buffer_size * hot_rate_per_size,\n",
    "    graph.Resources.OutputBuffer: delivery_buffer_size * cold_rate_per_size,\n",
    "    graph.Resources.IngestRate: ingest_rate,\n",
    "    graph.Resources.DeliveryRate: delivery_rate,\n",
    "    graph.Resources.LTSRate: lts_rate\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate graph\n",
    "\n",
    "Generate a sequence with all HPSOs appearing roughly as often as we expect them in a real-life schedule. We then shuffle this list and generate a (multi-)graph of tasks from it.\n",
    "\n",
    "Note that in contrast to Francois' scheduler, the resource usage of every task is fixed up-front, therefore we need to declare certain key sizes here. Adjust as necessary in relation to the capacities (see below) to get the desired amount of parallelism between tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tsequence = 20 * 24 * 3600\n",
    "Tobs_min = 10 * 60\n",
    "\n",
    "hpso_sequence, Tobs_sum = graph.make_hpso_sequence(telescope, Tsequence, Tobs_min, verbose=True)\n",
    "print(\"{:.3f} d total\".format(Tobs_sum / 3600 / 24))\n",
    "random.shuffle(hpso_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_parallelism = 2\n",
    "\n",
    "t = time.time()\n",
    "nodes = graph.hpso_sequence_to_nodes(csv, hpso_sequence, capacities, Tobs_min, batch_parallelism=batch_parallelism)\n",
    "print(\"Multi-graph has {} nodes (generation took {:.3f}s)\".format(len(nodes), time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for node in nodes:\n",
    "        print(\"{} ({}, t={} s)\".format(node.name, node.hpso, node.time))\n",
    "        for cost, amount in node.cost.items():\n",
    "            if cost in graph.Resources.units:\n",
    "                unit, mult = graph.Resources.units[cost]\n",
    "                print(\" {}={:.2f} {}\".format(cost, amount/mult, unit))\n",
    "        for cost, amount in node.edge_cost.items():\n",
    "            if cost in graph.Resources.units:\n",
    "                unit, mult = graph.Resources.units[cost]\n",
    "                print(\" -> {}={:.2f} {}\".format(cost, amount/mult, unit))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity-check Graph\n",
    "\n",
    "We can do a number of consistency checks at this point: Clearly we should have enough capacity to run every task in isolation.\n",
    "\n",
    "Furthermore, in order to keep up with observations we need to make sure that we are not over-using any resource on average. This is a pretty rough estimate of safety that especially under-estimates the cost of edges in high-pressure scenarios. For example, if somethings needs to be kept in the buffer for longer, it has a higher footprint than estimated here. Therefore especially the size of `input-buffer` and `output-buffer` should be quite generous here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_sum = { cost : 0 for cost in capacities.keys() }\n",
    "for task in nodes:\n",
    "    for cost, amount in task.all_cost().items():\n",
    "        assert cost in capacities, \"No {} capacity defined, required by {}!\".format(cost, task.name)\n",
    "        assert amount <= capacities[cost], \"Not enough {} capacity to run {} ({:g}<{:g}!)\".format(\n",
    "            cost, task.name, capacities[cost], amount)\n",
    "        # Try to compute an average. Edges are the main wild-card here: We only know that they stay\n",
    "        # around at least for the lifetime of the dependency *and* the longest dependent task.\n",
    "        ttime = task.time\n",
    "        if cost in task.edge_cost and len(task.rev_deps) > 0:\n",
    "            ttime += max([d.time for d in task.rev_deps])\n",
    "        cost_sum[cost] += ttime * amount\n",
    "print(\"Best-case average loads:\")\n",
    "for cost in graph.Resources.All:\n",
    "    unit, mult = graph.Resources.units[cost]\n",
    "    avg = cost_sum[cost] / Tobs_sum\n",
    "    cap = capacities[cost]\n",
    "    print(\" {}:\\t{:.3f} {} ({:.1f}% of {:.3f} {})\".format(cost, avg/mult, unit, avg/cap*100, cap/mult, unit))\n",
    "    # Warn past 75%\n",
    "    if avg > cap:\n",
    "        print('Likely insufficient {} capacity!'.format(cost), file=sys.stderr,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule tasks\n",
    "\n",
    "Assign a task time to every node, and figure out resource usages and edge lengths along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "usage, task_time, task_edge_end_time = scheduler.schedule(nodes, capacities, verbose=False)\n",
    "print(\"Scheduling took {:.3f}s\".format(time.time() - t))\n",
    "print(\"Observing efficiency: {:.1f}%\".format(Tobs_sum / usage[graph.Resources.Observatory].end() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_end = max(*task_edge_end_time.values())\n",
    "pylab.figure(figsize=(16,16)); pylab.subplots_adjust(hspace=0.5)\n",
    "for n, cost in enumerate(graph.Resources.All):\n",
    "    levels = usage[cost]\n",
    "    avg = levels.average(0,trace_end)    \n",
    "    unit, mult = graph.Resources.units[cost]\n",
    "    pylab.subplot(len(usage), 1, n+1)\n",
    "    pylab.step([0] + [ t/24/3600 for t in levels._trace.keys() ] + [trace_end],\n",
    "               [0] + [ v/mult for v in  levels._trace.values() ] + [0],\n",
    "               where='post')\n",
    "    pylab.title(\"{}: {:.3f} {} average ({:.2f}%)\".format(\n",
    "        cost, avg/mult, unit, avg / capacities[cost] * 100))\n",
    "    pylab.xlim((0, trace_end/24/3600)); pylab.xticks(range(int(trace_end)//24//3600+1))\n",
    "    pylab.ylim((0, capacities[cost] / mult * 1.01))\n",
    "    pylab.ylabel(unit)\n",
    "    if n + 1 < len(graph.Resources.All):\n",
    "        pylab.gca().xaxis.set_ticklabels([])\n",
    "pylab.xlabel(\"Days\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency calculations\n",
    "\n",
    "We can play around with capacities and see how it affects overall efficiency. This takes quite a bit, so let's set up some multiprocessing infrastructure to take advantage of parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "interesting_costs = [ graph.Resources.BatchCompute, graph.Resources.InputBuffer,\n",
    "                      graph.Resources.HotBuffer, graph.Resources.OutputBuffer]\n",
    "linked_cost = {\n",
    "    graph.Resources.HotBuffer : graph.Resources.HotBufferRate,\n",
    "    graph.Resources.InputBuffer : graph.Resources.InputBufferRate,\n",
    "    graph.Resources.OutputBuffer : graph.Resources.OutputBufferRate,\n",
    "}\n",
    "# Assumed price to add capacity\n",
    "cost_gradient = {\n",
    "    graph.Resources.BatchCompute : 1850000 / Constants.peta,\n",
    "    graph.Resources.RealtimeCompute : 1850000 / Constants.peta,\n",
    "    graph.Resources.HotBuffer    : 80000 / Constants.peta,\n",
    "    graph.Resources.InputBuffer  : 45000 / Constants.peta,\n",
    "    graph.Resources.OutputBuffer : 45000 / Constants.peta,\n",
    "}\n",
    "# Assumed price of entire telescope to assign cost to inefficiences\n",
    "total_cost = 250*Constants.mega\n",
    "@interact_manual(costs=SelectMultiple(options=graph.Resources.All, value=interesting_costs),\n",
    "                 percent=(1,100,1), percent_step=(1,10,1), count=(1,100,1), yaxis_range=(1, 20, 1),\n",
    "                 batch_parallelism = (1,10,1))\n",
    "def test_sensitivity(costs=interesting_costs, percent=50, percent_step=5, count=multiprocessing.cpu_count(),\n",
    "                     batch_parallelism=batch_parallelism, yaxis_range=5, cost_change=False):\n",
    "    # Calculate\n",
    "    lengths = efficiency.determine_durations_batch(csv,\n",
    "        hpso_sequence, costs, capacities, update_rates, percent, percent_step, count,\n",
    "        Tobs_min=Tobs_min, batch_parallelism=batch_parallelism)\n",
    "    # Make graph\n",
    "    graph_count = len(costs)\n",
    "    pylab.figure(figsize=(8,graph_count*4))\n",
    "    pylab.subplots_adjust(hspace=0.4)\n",
    "    for graph_ix, cost in enumerate(costs):\n",
    "        pylab.subplot(graph_count, 1, graph_ix+1)\n",
    "        efficiency.plot_efficiencies(pylab.gca(), Tobs_sum, cost, capacities[cost], lengths[cost],\n",
    "                                     linked_cost.get(cost), update_rates,\n",
    "                                     cost_gradient.get(cost) if cost_change else None, total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with failures\n",
    "\n",
    "It is also possible to simulate failures. The idea is that we model this as a temporary capacity change, which requires us to re-schedule twice (once the capacity reduces, and once it's restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.schedule(nodes, capacities, task_time, task_edge_end_time, verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_capacities = dict(capacities)\n",
    "new_capacities[graph.Resources.InputBuffer] = capacities[graph.Resources.InputBuffer] // 2\n",
    "usage2, task_time2, task_edge_end_time2, failed_usage2 = scheduler.reschedule(\n",
    "    nodes, new_capacities, 5*24*3600, task_time, task_edge_end_time, verbose=False)\n",
    "usage3, task_time3, task_edge_end_time3, failed_usage3 = scheduler.reschedule(\n",
    "    nodes, capacities, 8*24*3600, task_time2, task_edge_end_time2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_end = max(*task_edge_end_time3.values())\n",
    "pylab.figure(figsize=(16,16)); pylab.subplots_adjust(hspace=0.5)\n",
    "for n, cost in enumerate(graph.Resources.All):\n",
    "    levels = usage3[cost]\n",
    "    avg = levels.average(0,trace_end)    \n",
    "    unit, mult = graph.Resources.units[cost]\n",
    "    pylab.subplot(len(usage), 1, n+1)\n",
    "    for levels in [failed_usage2[cost] + failed_usage3[cost] + usage3[cost], failed_usage2[cost] + failed_usage3[cost]]:\n",
    "        pylab.step([0] + [ t/24/3600 for t in levels._trace.keys() ] + [trace_end],\n",
    "                   [0] + [ v/mult for v in  levels._trace.values() ] + [0],\n",
    "                   where='post')\n",
    "    pylab.title(\"{}: {:.3f} {} average ({:.2f}%)\".format(\n",
    "        cost, avg/mult, unit, avg / capacities[cost] * 100))\n",
    "    pylab.xlim((0, trace_end/24/3600)); pylab.xticks(range(int(trace_end)//24//3600+1))\n",
    "    pylab.ylim((0, capacities[cost] / mult * 1.01))\n",
    "    pylab.ylabel(unit)\n",
    "    if n + 1 < len(graph.Resources.All): pylab.gca().xaxis.set_ticklabels([])    \n",
    "pylab.xlabel(\"Days\")\n",
    "pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
